{
  "noteRecords": [
    {
      "title": "intro to python",
      "note_id": "c5beeea6-4b22-40c5-a6bb-5b2f19132a59",
      "markdown": "# intro to python\n\n## Overloaded Operators\n<2024-01-15T04:41:49.172Z> \n- Operators can be overloaded, allowing them to behave differently based on their context.\n- Example: combining a number with a string using multiplication will result in a new string with the specified number of replicas.\n- Example: two strings can be combined using the addition operator.\n\n## Static Semantics\n<2024-01-15T05:11:23.224Z>\nIn the context of programming, static semantics is highlighted as an important factor. It involves the analysis of a program's text to determine its meaning, even before it is run.\n\n\n## Type Checking in Python\n<2024-01-15T05:12:24.228Z>\n- Python performs type checking to detect data type-related errors.\n- Inconsistencies in type checking can lead to issues downstream.\n- Different programming languages exhibit a spectrum of type checking, ranging from weak to strong typing.\n\n<2024-01-15T05:13:16.223Z>\nThe lecturer highlights the differences in type checking in Python compared to other languages, especially those with weak typing.\n\n<2024-01-15T05:14:19.224Z>\nThe lecturer delves into an example to demonstrate Python's type checking behavior using the \"less than\" operator, where unexpected comparison outcomes occur between strings and numbers.\n\n<2024-01-15T05:15:19.227Z>\nThe lecturer emphasizes the importance of exercising type discipline in coding to ensure the correct usage of operators and operands.\n\n<2024-01-15T05:17:02.223Z>\nFurther discussion involves the unconventional meanings of certain operators in Python, such as division and the order of operations for arithmetic expressions.\n\n<2024-01-15T05:18:10.218Z>\nThe lecturer continues the discussion about the order of operations, emphasizing the need to be explicit when specifying the desired order of evaluation in expressions.",
      "status": "inactive"
    },
    {
      "title": "MIT lecture 3",
      "note_id": "7dfafac7-faec-4c06-987f-40e4f4e1cee6",
      "markdown": "# MIT lecture 3\n\n## Programming Basics\n<2024-01-15T05:19:32.217Z> \nThe lecturer begins by summarizing the content covered in the previous lectures. They outline the basic elements of programming, including data (numbers, strings, and booleans), operations (such as addition and multiplication applicable to numbers and strings), and commands/statements for changing the flow of control. They also introduced loop mechanisms and emphasized the power of these fundamental instructions for writing common patterns of code.\n\n## Importance of Good Programming Style\n<2024-01-15T05:23:13.783Z>\nThe lecturer highlighted the significance of good programming style, emphasizing the following key points:\n- Use of comments for code clarity and debugging.\n- Type discipline to ensure expected types for operations.\n- Descriptive and meaningful variable names.\n- Testing all possible branches within the code.\nThe focus was on promoting good programming practices to write high-quality code.\n\n\n## Iterative Programs\n<2024-01-15T05:24:32.240Z>\nThe lecture concluded with a hint at building upon the basics discussed previously. The lecturer indicated the intention to delve into iterative programs, focusing on patterns of code that tackle certain classes of problems and tools to aid in understanding those pieces of code. Emphasis was placed on the selection of variables for counting within a process.\n\n<2024-01-15T05:25:24.748Z>\nThe lecturer outlined the key steps for creating an iterative program: Initializing the variable, setting up the right end test to determine loop completion, ensuring the block of code inside the loop contains instructions that change the variable being counted, and deciding what to do when the loop is done.\n\n<2024-01-15T05:26:25.548Z>\nA reminder was given that the block of code in an iterative program represents the set of instructions to be executed each time through the loop and highlighted the necessity of changing the counting variable within the block of code to avoid infinite loops. The importance of having a clear structure when mapping a problem into an iterative program was emphasized.\n\nThis section gave a high-level overview of the considerations and steps involved in creating iterative programs, with a focus on the manipulation of variables within the iterative process.",
      "status": "inactive"
    },
    {
      "title": "ML3",
      "note_id": "f9c649d2-8423-4368-a41a-e7ee7f24e65b",
      "markdown": "# ML3 \n\n## Machine Learning Review\n2024-01-17T01:46:35.872Z\n- Supervised learning involves feature extraction and learning algorithms\n- Feature extraction converts examples into a feature vector\n- Linear predictors or neural networks provide scores for the feature vectors\n- Scores are defined via simple or more complex linear combinations\n- Score functions can be used for classification and regression\n\n## Back Propagation Example and K Nearest Neighbors\n2024-01-17T01:46:35.872Z\n- Back propagation exemplifies how neural networks update weights during training.\n- K Nearest Neighbors is an algorithm used for classification and regression based on the proximity of data points.\n\n## True Objective of Machine Learning\n2024-01-17T01:47:35.473Z\n- Minimizing error on unseen future examples\n- Learning about machines\n- **Machine Learning Vocabulary**\n    - *Gradient*: A vector that points in the direction of the greatest rate of increase of a function and whose magnitude is the slope of the function in that direction.\n- The true objective is minimizing error on unseen future examples, rather than just minimizing error on the training set.\n- Unsupervised learning will be discussed later.\n\n\n## Training Loss as an Objective Function\n2024-01-17T01:51:21.884Z\n- Switching gears to talk about unsupervised learning.\n- Questioning whether training loss is a good objective function.\n- The described algorithm suggests outputting the weight seen in the training set, indicating a need for objective functions beyond simply minimizing training loss.",
      "status": "inactive"
    },
    {
      "title": "ITP 359 lecture 3 Review of supervised ML",
      "note_id": "1773f188-fafe-463e-861d-56fea9219a4f",
      "markdown": "# ITP 359 lecture 3 Review of supervised ML\n\n## Troubleshooting Testing (2024-01-17T02:00:52.619Z)\n- Testing was done before arrival but encountered an error.\n- Describing need for uninterrupted functioning and optimization.\n- Looking to address refreshing issue when using effects and location hook.\n- Working on proper re-rendering of the system.\n- Referring to seller's rooms in the discussion.\n\n## Testing and Error Handling\n- Testing the functionality before the official use to identify and resolve any potential errors.\n- In this context, \"error\" refers to an unexpected problem or malfunction in the system.\n\n## Testing and Debugging\n2024-01-17T02:00:52.619Z\n- Testing a feature before it's ready\n- Encountering errors while testing\n- Needing to optimize the feature\n- Difficulty with refreshing and re-rendering when using location hook to pull data from a link\n\n## Review of Last Semester - 2024-01-17T02:02:50.993Z\n- Next week's session will focus on forecasting.\n- The upcoming topics include Markov chains and naive Bayes outboard.\n- These concepts are important statistical methods.\n\n## Sequential Data in Finance and Audio Processing\n2024-01-17T02:04:42.744Z\n- Algorithmic traders often use sequential data for trading, which is also utilized in audio processing.\n- Hidden Markov models, autoencoders, and variational autoencoders are popular in processing sequential data for various applications.\n\n## AI Approaches and Modeling Techniques\n2024-01-17T02:06:10.989Z\n- Generated AI mentions the use of generators in AI models\n- Plans to start with variation oil and odors in those types of models\n- Homework on forecasting to be completed within two weeks\n- The first homework may be released on Thursday or Monday/Tuesday\n- Emphasis on not rushing through the material and having ample time for learning\n\n## Approximation Problem and AI Approaches\n2024-01-17T02:07:39.014Z\n- **Approximation Problem:** A problem involving finding a function that approximates a given target, often tackled using statistical or numerical methods.\n- AI approaches primarily involve dealing with approximation problems.\n- Techniques for addressing approximation problems include statistical methods, numerical methods, linear regression, logistic regression, and neural networks.\n\n## Regression and Classification\n2024-01-17T02:09:09.171Z\n- Neural network used for classification and regression\n- K-Nearest Neighbor algorithm used for classification and recommendation systems\n- Brief intro to Association Analysis and Principal Component Analysis (PCAs)\n\n## Traveling Salesman Problems and Research Competition\n2024-01-17T02:10:14.064Z\n- Traveling Salesman Problems involve finding the shortest possible route that visits each city exactly once and returns to the original city\n- Research competition likely involved exploring algorithms and strategies for solving complex problems like the Traveling Salesman Problem\n- Statistical standpoint refers to analyzing data from a mathematical perspective, considering patterns and probability\n- Machine learning standpoint involves using algorithms and statistical models for computers",
      "status": "inactive"
    },
    {
      "title": "ITP 359 lecture 4 Neural Networks",
      "note_id": "856f1b5f-39b5-48c9-a549-b526b76ef6bc",
      "markdown": "# ITP 359 lecture 4 Neural Networks \n\n## Neural Networks and Approximators <2024-01-17T03:15:13.293Z>\n- Neural networks as approximators - The neural network is used as an approximator by approximating a function with the help of learning from training examples.\n- Basic idea of neural networks - Each node in the network uses a linear combination of feature vectors and associated weights.\n- Vocabulary: \n  - **Approximator**: In machine learning and statistics, an approximator is a method or learning algorithm that seeks to approximate a target function through training on example inputs, and adjusting the model's parameters to minimize the difference between the predicted and actual outputs.\n\n## Understanding the Perceptron Training Process\n2024-01-17T03:20:32.067Z\n- Perceptron training minimizes misclassifications\n- Misclassifications are minimized using absolute value\n- Weight updating is based on data points and their associated calculations\n- Learning rate multiplied by the distance between the approximation and the actual value\n\n## Training a model\n2024-01-17T03:21:48.840Z\n- Start with the weights being small, not too big to avoid issues with training.\n- Perform steps for each data point, including calculating the linear combination, applying an activation function, and updating the weights.\n\n## Perceptron Training Process \n2024-01-17T03:22:59.749Z\n- Updating weights in each step for each data point\n- Calculating the error and updating weights until a very small error is achieved\n\n## Linearly Classifiable Data and Neural Networks\n2024-01-17T03:25:44.088Z\n- Linearly classifiable data: Data that can be separated into different classes using a straight line or hyperplane\n- Neural networks: A series of algorithms that endeavors to recognize underlying relationships in a set of data through a process that mimics the way the human brain operates\n\n## New AI Architecture\n2024-01-17T03:27:15.177Z\n- The new AI architecture on Netflix involves a dual network with multiple layers. \n- It utilizes activation functions such as sigmoid function, real or logistics function, and hyperbolic tan function to replace the step function.\n- The architecture emphasizes the use of non-linearities in constructing multilayer neural networks.\n\n## Neural Network Definition\n2024-01-17T03:29:09.085Z\n- The network is defined as starting at the end and working backward, with the output being an activation function of the output weights times the output of the final layer. \n- Each layer is defined by a number of \"Lambda\" nodes, with all nodes connected to each other, creating a fully connected feedforward network.\n\n## Neural Network Structure\n2024-01-17T03:32:14.821Z\n- The output of a neural network depends non-linearly on the last layer, and this layer depends on the previous layer and so on, creating a recursive structure within the network.\n- The network can be visualized as an onion, starting with the output and then working back to the input, with calculations being made as it progresses backward through the layers.\n\n## Universal Approximation Theorem\n2024-01-17T03:33:17.509Z\n- The universal approximation theorem, first articulated in 1989, states that a feedforward neural network with a single hidden layer containing a finite number of neurons can approximate any continuous function on compact subsets.\n- Compact subsets are subsets that include their boundary and do not have to satisfy specific constraints on their nature.\n\n## Multi-class Classification and Softmax\n2024-01-17T03:35:46.043Z\n- The raw output of the network is passed through a sigmoid function, mapping the values to a range between 0 and 1, to obtain the probability for each class in a multi-class classification.\n- The issue with using the sigmoid function alone is that the probabilities assigned to each class do not necessarily add up to one, hence creating ambiguity in classification.\n- This method is referred to as multi-class classification, where multiple classes can be associated with the input.\n\n## Sigmoid and Softmax Functions\n2024-01-17T03:37:29.206Z\n- Sigmoid function used in final layer for multi-class classification\n    - Sigmoid function does not ensure probabilities add up to one\n    - Allows for classification into multiple classes\n- Softmax function exaggerates differences in probabilities and ensures sum of probabilities equals one\n    - Enables exclusive belonging or mapping to a class\n\n## Softmax and Multi-Class Classification\n2024-01-17T03:38:30.789Z\n- Softmax ensures that the output values add up to one, which is essential in binary classification to obtain a classification probability.\n- In multi-class classification, you may want to compare probabilities between different outcomes, such as distinguishing between various objects in an image.\n- Softmax exaggerates the distances between classes, making them not directly comparable, which might not be suitable if you want to rank the different classes based on their probabilities.\n\n## Softmax and Cross Entropy Loss\n2024-01-17T03:40:46.085Z\n- Softmax: A function that takes as input a vector of K real numbers, and normalizes it into a probability distribution consisting of K probabilities.\n- Softmax is used in neural networks to provide probabilities for the different classes in a classification problem.\n- Cross Entropy Loss: A loss function used in neural networks when the output is a probability value. It measures the performance of a classification model whose output is a probability value between 0 and 1. \n\n## Cross Entropy Loss\n2024-01-17T03:42:03.824Z\n- Cross entropy loss is very similar to the negative log likelihood and is used in models.\n- In the case of two classes, cross entropy loss becomes the same as the negative log likelihood equation.\n- The derivation of cross entropy loss comes from taking the logarithm and maximizing the likelihood function.\n\n\n## Softmax Activation Function\n2024-01-17T03:43:11.596Z\n- The softmax activation function is used as the final activation function, providing output probabilities that add up to one.\n- It helps in determining a clear winner among the classes in the model.",
      "status": "inactive"
    },
    {
      "title": "Descriptive Data Mining",
      "note_id": "d2ff3d8b-92dc-45ca-8152-81736e53a2f6",
      "markdown": "# Descriptive Data Mining \n\n## Data Mining\n<2024-01-17T17:08:45.764Z> \n- Data mining is the process of analyzing large sets of data to discover patterns, trends, and relationships.\n- With the advancement of technology, we now have the capability to extract valuable insights from the vast amount of data that is being generated.\n\n## Data Mining\n2024-01-17T17:11:47.830Z\n- Data mining is the process of analyzing large sets of data to discover patterns, trends, and relationships.\n- It involves extracting useful information from a large amount of data, often using automated methods or algorithms.\n- Companies use data mining to gain insights into customer behavior, market trends, and other business-related information.\n\n## Interest Rate Period\n2024-01-17T17:11:47.830Z\n\n- Interest rate period is the duration for which a specific interest rate is applicable for a financial product.\n- In AI, data is highly valuable for companies to derive insights and make informed decisions. \n- The process of mining and analyzing data allows companies to extract actionable information and drive value. \n\n## Algorithm and Qualitative Assessments\n2024-01-17T17:13:02.993Z\n- Algorithm: A set of rules or processes to be followed in calculations or problem-solving operations, especially by a computer.\n- Qualitative assessments: Evaluations that are based on qualities or characteristics rather than on quantity or numerical data.\n- Consider the use of qualitative assessments in decision-making processes.\n- Discuss the potential impact of qualitative assessments on business decisions.\n\n## Cluster Analysis and Similarity\n2024-01-17T17:14:11.304Z\n- **Cluster Analysis**: A method of grouping objects based on the similarity of their attributes or characteristics.\n- Data preparation is underway.\n- Two ways of dealing with daily money have been mentioned.\n- Mention of clustering of similarity for computer reports and association rules.\n\n## Computer Grouping and Data Analysis\n- The process of grouping data for significance and analysis.\n- Utilizing computer algorithms to group similar data together for analysis and decision-making.\n- Methods for determining similarity and measuring distance between data points.\n\n## Grouping data and measuring similarity \n- Date-time: 2024-01-17T17:15:36.708Z\n- The data is grouped to find significance and aid in decision-making processes.\n- The computer can identify similarities between data using numerical comparisons.\n- Similarity can be measured by comparing numbers, such as age and income, using methods like z-scores and standard deviation.\n\n## Scale and Range of Values\n2024-01-17T17:17:59.707Z\n- Range of values: The difference between the minimum and maximum values within a dataset.\n- Age versus income scale: Income is in the tens of millions while age has a lesser range.\n- Z-scores: Standardized scores that measure the distance of a data point from the mean in terms of standard deviations.\n- Using z-scores to fix scale issue and bring values to the same scale for comparison.\n\n## Z-Scores Explained\n2024-01-17T17:19:44.396Z\n- Z-Score: A measurement of a value's relationship to the mean of a group of values, measured in terms of standard deviations. It indicates how many standard deviations a data point is from the mean.\n- Discussion of using Z-scores to analyze income data and how it can affect the distance equation in analysis.\n\n## Analysis of Distance and Z-scores\n- 2024-01-17T17:20:55.708Z\n- The computation of distance between data points is essential in analysis\n- Z scores dominate the distance equation, which can skew the analysis\n- Z scores transform data values to represent their distance from the mean\n\n## Standard Deviation and Z-Score\n2024-01-17T17:22:09.712Z\n- **Standard deviation**: A measure of the amount of variation or dispersion of a set of values. It illustrates how much individual data points differ from the mean.\n- **Z-Score**: A statistical measurement that describes a value's relationship to the mean of a group of values and quantifies how many standard deviations above or below the mean a data point is.\n- The discussion pertains to using Z-score values to determine the distance between data points and how it relates to clustering analysis.\n\n## Floating Point Numbers and Categorical Data\n2024-01-17T17:23:25.406Z\n- **Floating point numbers** are numbers that contain a decimal point. They can represent a wide range of values.\n- **Categorical data** refers to data that represents categories or groups, such as colors or types of fruit.\n\n## K-means Clustering\n2024-01-17T17:24:45.638Z\n- **K-means**: A popular unsupervised machine learning algorithm used for clustering data points into a pre-defined number of clusters.\n- Comparing data points to find clusters for poster creation, based on the nearest neighbors.\n- Exploring the process of creating clustered groups on the ground.\n\n## K Means Clustering\n2024-01-17T17:25:41.736Z\n- K Means Clustering: a type of unsupervised learning algorithm used to group similar data points into a fixed number (k) of clusters.\n- The algorithm iteratively assigns each data point to the nearest of k centroids and then recalculates the centroids based on the mean of the assigned data points.\n- Involves choosing the number of clusters, initializing centroids, assigning data points to the nearest centroid, and updating centroids.\n\n## Hierarchical Clustering\n2024-01-17T17:26:51.716Z\n- **Hierarchical clustering**: A method of cluster analysis which seeks to build a hierarchy of clusters. \n- Hierarchical clustering can be advantageous when working with small datasets, as it enables the formation of clusters from individual observations, but it may not be suitable for large datasets as it can become computationally intensive.\n- Outliers can greatly impact the output of hierarchical clustering. The presence of outliers can affect the distance measurements and ultimately influence the clustering process.\n\n## Hierarchical Clustering and Data Preprocessing\n2024-01-17T17:28:04.717Z\n- Hierarchical clustering: A method of cluster analysis that builds a hierarchy of clusters by either merging or splitting them successively.\n- Outliers removal: The process of eliminating data points that significantly differ from the rest of the data to improve analysis accuracy.\n- Handling large datasets: Hierarchical clustering may not be efficient for datasets with thousands or millions of data points.\n\n## Data Mining and Clustering Analysis\n- Data mining is the process of analyzing large data sets to identify patterns and relationships.\n- Cluster analysis is a technique used to group similar data points together based on certain characteristics.\n\n## Data Mining and Cluster Analysis\n2024-01-17T17:31:28.691Z\n- Data mining is the process of analyzing large sets of data to discover patterns or relationships.\n- Cluster analysis is a technique used to group similar items or data points together.\n\n## Converting Language to Numbers\n2024-01-17T17:40:33.694Z\n- **Unstructured Data**: Data that isn't organized in a pre-defined manner and doesn't have a specific data model. \n- Computer processing text requires converting it into structured numerical data for analysis.\n- The process involves identifying important information from the unstructured data to create a set of numbers for analysis.\n\n## Structuring Text Data for Analysis\n2024-01-17T17:44:11.693Z\n- We need to structure the text data to be something that can be plugged into SPSS, Rosen Collins, or equivalent tools.\n- We can create a term document matrix where important words are picked out and presented to the computer as a set of words.\n- A term document matrix allows us to pick out important words and provide this information to the computer, enabling analysis.\n\n## Structuring the Data for Analysis\n2024-01-17T17:44:11.693Z\n- Term Document Matrix: A way to represent the words in a document as rows and the documents themselves as columns. \n- SPSS and Rosen Collins: Refers to statistical analysis software used for processing survey data.\n- The speaker is discussing the process of creating a term document matrix to analyze survey responses.\n\n## Clustering of Textual Data\n2024-01-17T17:45:25.390Z \n- The process involves analyzing the frequency of words in a set of data to identify patterns and similarities.\n- Tokenization is the process of breaking text into individual words or tokens for analysis and is a crucial step in clustering textual data. \n\n## Tokenization and Text Preprocessing\n2024-01-17T17:46:50.004Z\n- **Tokenization**: Process of breaking text into individual words or tokens for analysis.\n- Remove non-word characters and punctuation marks from text data.\n- Convert all words to lowercase to ensure consistent analysis.\n- Identify and handle word endings to unify related terms.\n\n## Identifying and Processing Words\n- **Stacking**: The act of placing one item on top of another.\n- **Algorithm**: A step-by-step procedure for solving a problem or accomplishing a task, often used in computer programming and artificial intelligence.\n\n## Discussion on Natural Language Processing\n2024-01-17T17:48:05.422Z\n- Natural Language Processing (NLP) - a field of AI focused on enabling computers to understand, interpret, and manipulate human language.\n- Importance of standardizing text - converting all text to lowercase can help in simplifying the processing of data and reduce the variations due to upper/lower case.\n\n\n## Data Preprocessing and Text Analysis\n2024-01-17T17:49:07.485Z\n- Word Stemming - the process of reducing words to their root forms, such as converting \"stacking\" to \"stack\" and \"stacked\" to \"stack,\" to facilitate analysis.\n- Handling synonyms - discussing the transformation of similar words to a common representation, e.g., \"courteous\" and \"cordial\" both being transformed to \"polite,\" for more consistent analysis.\n- Removal of rare words - considering the insignificance of words occurring very infrequently, also referred to as outliers, to ensure meaningful and relevant data analysis.\n\n## Creation of Frequency Matrix\n2024-01-17T17:50:10.660Z\n- Creation of a frequency matrix - a tabular representation showing the occurrences of specific words (e.g., \"great,\" \"terrible\") in the dataset, aiding in data summarization and analysis.\n- Decisions based on word frequencies - using the frequency data to make decisions, such as understanding trends, progress, and changes over time based on the occurrences of certain terms.\n\n## Utilizing data for decision making\n2024-01-17T17:51:03.688Z\n- Decision-making based on data - leveraging the processed data to make informed decisions, possibly on a regular basis, to assess progress, trends, and changes in specific terms and overall data analysis.",
      "status": "inactive"
    },
    {
      "title": "Lecture 1: Introduction to Superposition",
      "note_id": "5f50045b-1c0a-4be9-bbff-dc92bc6c86be",
      "markdown": "# Lecture 1: Introduction to Superposition \n\n## Introduction to the Lecture\n2024-01-18T17:42:50.630Z\n- Alan Adams is an assistant professor in course eight, specializing in string theory and its applications to gravity and condensed matter physics.\n- He is excited to teach the course and mentions that quantum mechanics is his daily language.\n\n## Introduction to the Course\n2024-01-18T17:42:50.630Z\n- Alan Adams, assistant professor in course eight, will be teaching the course on string theory and its applications to gravity and condensed matter physics.\n- The course will heavily focus on quantum mechanics.\n\n## Lecture notes, homeworks, exams on Stellar website\n2024-01-18T17:43:49.027Z\n- All course materials, including lecture notes, homework, and exams, will be accessible through the Stellar website.\n  \n## Quantum Mechanics\n2024-01-18T17:44:55.031Z\n- Quantum mechanics is a branch of physics that focuses on the behavior of particles at very small scales, such as the atomic and subatomic levels.\n\n## Course Structure and Expectations\n2024-01-18T17:45:49.027Z\n- The course will entail weekly problem sets, with the possibility of dropping one to account for unanticipated events.\n- Collaboration with other students is encouraged, including working together to solve problems.\n- There will be two midterms, with dates to be announced, as well as a final exam.\n- Clickers will be used and required for participation, although attendance will not be taken.\n\n\n## Utilization of Clickers\n2024-01-18T17:46:34.025Z\n- The organization of the course is based on empirical lessons on effective teaching methods.\n- Clickers will be used as part of the instruction, and students are required to have and register them on the tsg website by the next week.\n- **Clickers**: A classroom response system that allows students to respond to questions electronically.\n- **Empirical**: Based on evidence from observation or experience rather than theory.",
      "status": "inactive"
    },
    {
      "title": "Lecture 2: Experimental Facts of Life",
      "note_id": "c83f880d-7975-45e6-9c75-144754cd06cb",
      "markdown": "# Lecture 2: Experimental Facts of Life \n\n\n## Basic Experimental Facts in Physics\n<2024-01-18T19:20:19.476Z>\n- Physics provides models to understand the universe, rather than abstract truths.\n- Adams exist, supported by various arguments.\n- Randomness is definitely present in the world.\n- The photoelectric effect is structured and observable.\n\nDo you need any definitions of AI vocabulary or additional information on any specific topic related to physics?",
      "status": "inactive"
    },
    {
      "title": "Competition is for Losers with Peter Thiel",
      "note_id": "2979733f-ad5a-4810-9479-edc65b7e4f03",
      "markdown": "# Competition is for Losers with Peter Thiel \n\n## Creating Value in Business\n<2024-01-18T22:14:14.470Z>\n- The basic idea of creating value in business is essential for understanding the foundation of businesses.\n- Creating value in business involves two key elements:\n  - **Creating Something of Value**: Generating a product or service that has worth and usefulness.\n  - **Capturing Some Fraction of Value**: Securing a portion of the value created to make the business profitable.\n- An illustration of this concept can be seen in comparing the US airline industry with a company like Google in the context of size and revenue generation.\n\n## Creating Value and Capturing Value\n2024-01-18T22:14:14.470Z\n- Value creation and capture are fundamental to the success of a business\n- Value creation: Generating something of worth\n- Value capture: Obtaining a portion of the created value\n- Example: Comparison of US airline industry and Google's search business to demonstrate the importance of both value creation and capture\n\n## Air Travel vs Search Engines\n2024-01-18T22:15:16.468Z\n- Is air travel more important than the use of search engines?\n    - Domestic numbers suggest air travel is more significant.\n- Globally, airlines have a much larger market capitalization than search engines like Google.\n- The combined market capitalization of the US airline industry is approximately a quarter of that of Google.\n\n## Perfect competition and monopoly\n2024-01-18T22:16:30.466Z\n- Perfect competition: a market structure in which a large number of firms all produce the same product and no single seller can influence the market price.\n- Monopoly: a market structure in which a single firm is the sole producer or seller of a product.\n\n## Understanding Business Lies\n2024-01-18T22:18:03.470Z\n- Businesses lie about their competitiveness.\n- Spectrum of companies exist from perfect competition to Monopoly.\n- Monopolies pretend to be incredibly competitive to avoid government regulation.\n- Monopolies pretend not to be monopolies to avoid being targeted.\n\n## Understanding Monopolies and Competitive Markets\n2024-01-18T22:19:42.486Z\n- **Monopoly**: A market structure characterized by a single seller and no close substitutes, giving the monopolist significant control over the price of the product.\n- Differentiation is important to track capital and compete effectively.\n- Example: A monopolist describes their business as the union of vastly different markets, making their market seem much bigger, while a non-monopoly describes it as the intersection, making their market seem super small with less competition.\n\n## Understanding Perfect Competition\n2024-01-18T22:21:10.676Z\n- In perfect competition, all market participants have the same level of information and resources, leading to an equal opportunity for success.\n- Capital accumulation in the context of perfect competition refers to the acquisition of financial and physical assets for future profit.\n\n## Buzzword Sharing Mobile Social Apps\n2024-01-18T22:23:19.473Z\n- Buzzword: A term that becomes popular in a specific industry or field for a period of time.\n- Mobile social apps are applications that allow users to interact and share content with others on their mobile devices.\n\n## Technology Market Overview\n2024-01-18T22:24:34.730Z\n- The technology market is a significant, approximately 1 trillion dollar market\n- Google competes with various companies in different segments, such as car companies with self-driving cars, Apple on TVs and iPhones, and Amazon on cloud services\n- There is a concern about potential regulations due to the market's size and Google's competitive presence\n\n## Tech Industry and Monopoly-like Businesses\n2024-01-18T22:25:29.293Z\n- Tech industry in the US has been financially successful due to the creation of monopoly-like businesses.\n- Startups are advised to target small markets to achieve eventual monopoly status.\n- Companies expand their market presence in concentric circles over time.\n- Going after a giant market from day one is a common mistake as it indicates incorrect category definition and excessive competition.\n\n## Building a Monopoly\n2024-01-18T22:26:47.514Z\n- Monopoly: A market structure where a single company dominates the market and has significant market power.\n- Starting a new company with the goal of achieving monopoly.\n- Starting with a small market and gradually expanding in concentric circles.\n- Avoiding going after a giant market on day one to minimize competition.\n- Examples of successful companies like Amazon, eBay, and Facebook starting with small markets and growing over time.\n\n## PayPal Version of Start-up Strategy\n2024-01-18T22:28:10.479Z\n- Started with power sellers on eBay, around 20,000 people\n- Initially faced criticism for terrible customer service\n- Gained brand recognition after two or three months\n- Emphasizes the potential of underrated small markets\n- Small markets can pave the way for scalable business growth\n\n## Views on Market Strategy and Monopolies\n2024-01-18T22:30:23.474Z\n- In the energy market, being a small player among huge competitors can be challenging.\n- Advantages of being the only company in a small ecosystem.\n- Small markets that may seem insignificant at first could hold potential for expansion.\n\n## Reflections on Technology History\n2024-01-18T22:31:14.433Z\n- The history of technology often dictates unique business opportunities.\n- Emphasizes that the next groundbreaking technology will not necessarily replicate previous successes, but rather build on them uniquely.\n\n## AI Vocabulary\n- **Monopoly**: a market structure in which a single seller sells a unique product and there are barriers to entry for other potential competitors.\n- **Network effects**: the phenomenon where a product or service gains additional value as more people use it.\n- **Branding**: the process of creating a unique name and image for a product in the consumers' mind, typically through advertising campaigns and consistent messaging.\n\n## Happy Companies and Unhappy Companies\n2024-01-18T22:32:23.480Z\n- **Monopoly Technology Company**: A company with exclusive control over a product or service in a particular market.\n- Happy companies are unique and doing something very different.\n- Unhappy companies fail to escape the essential characteristic of a monopoly technology company.\n- The speed of transactions on platforms like eBay and PayPal is crucial for business success.\n\n## Monopoly power and discounted cash flow analysis\n2024-01-18T22:35:29.481Z\n- Monopoly power in business\n    - Refers to a situation where a company has exclusive control over a particular market or industry, allowing them to set prices and control supply\n- Discounted cash flow analysis\n    - A method used to evaluate the attractiveness of an investment opportunity by estimating the future cash flows and discounting them back to the present value\n\n## PayPal's Business Evaluation in 2001\n2024-01-18T22:36:40.480Z\n\n- PayPal was in business for about 27 months, experiencing a growth rate of 100% per year.\n- Discounted future cash flows by about 30%.\n- Three quarters of the value of the business in 2001 came from \"beyond.\"\n- Durability of growth is a measurable factor in tech companies' valuation.\n  \n## Value equation and Monopoly characteristics\n- The value equation refers to the qualitative assessment of value in a business context.\n- Monopoly characteristics may include proprietary technology, network effects, and economies of scale.\n- Proprietary technology refers to technology that is owned and controlled by a company, giving it a competitive advantage.\n- Network effects occur when the value of a product or service increases as more people use it.\n- Economies of scale refer to the cost advantages that a business obtains due to expansion.\n\n2024-01-18T22:38:56.484Z\n\n## Chess Strategy and Innovation\n2024-01-18T22:40:07.492Z\n- White advantage in chess: The player using white pieces has a slight advantage at the beginning of the game, about 1/3 of a pawn.\n- Last mover advantage: In chess, there is a concept called \"last mover advantage\" which suggests that the player who moves last has a better chance of winning.\n\n## Technological Progress and Innovation\n2024-01-18T22:40:51.490Z\n- Remarkable progress from steam engine to telephones to computer revolution and more.\n- Technological innovation has impacted various domains, including household appliances and aviation.\n\n## Lack of Rewards for Inventors and Innovators\n2024-01-18T22:42:21.486Z\n- The history of science shows a pattern of inventors not being adequately rewarded for their innovations\n- Not capturing the value of their inventions\n- Scientists and techn innovators not making money despite their groundbreaking work\n\n## Competition in Industrial History\n2024-01-18T22:43:52.491Z\n- The structure of industries and competition has been important for success.\n- In the 1850s, wealth in Britain was primarily held by the landed aristocracy.\n- Only two broad categories exist in the entire history of the last 250 years.\n\n## Coordination\n2024-01-18T22:45:37.495Z\n- Coordination: The act of organizing or integrating different components or processes in a way that they work together efficiently.\n- Integrating different pieces together and doing it in a way that is vertically integrated is valuable in business.\n- It is capital intensive and can be challenging to get people to buy into anything that is super complicated and takes a long time to build.\n- Tesla's approach involved integrating card distributors to prevent them from taking all the money.\n- Vertical integration is an underexplored modality of technological progress.\n\n## Vertical integration in aerospace industry\n2024-01-18T22:47:21.067Z\n- Large aerospace companies have single source subcontractors\n- These subcontractors charge monopoly profits, making it difficult for aerospace companies to make money\n- Vertical integration is an underexplored modality of technological progress\n\n## Rationalization\n2024-01-18T22:49:33.490Z\n- **Rationalization**: the action of attempting to explain or justify behavior or an attitude with logical reasons, even if these are not appropriate.\n- The speaker questions if a certain behavior is a rationalization to obscure the fact that there is no progress in a particular area. \n- The discussion delves into the impact of competition and the psychological bias towards it.\n\n## Reflection on Competition\n2024-01-18T22:51:02.494Z\n- **Distorted**: Twisted or misrepresented.\n- People are attracted to competition due to finding reassurance in it.\n- Validation is something that needs to be thought through, as the pursuit of something with lots of competition could be problematic.\n- Olympics are seen as better than the pursuit of becoming a movie star as it allows for a quick understanding of one's capabilities.\n\n## Competition and Differentiation\n2024-01-18T22:53:28.494Z\n- **Differentiate**: To recognize or express the difference between things.\n- When objective differences are small, fierce competition arises to maintain a perceived difference.\n- People's identities are often tied to winning competitions, overshadowing what is truly important and valuable.\n- Competition fosters improvement by comparing oneself to others, but may come at a significant cost.\n\n## Reflection on the Importance of Questioning and Exploration\n2024-01-18T22:55:19.497Z\n- Sometimes, focusing on immediate goals can prevent people from exploring bigger questions or alternatives.\n- It's advisable to consider alternative paths that may be less crowded but hold greater potential.\n\n\n## Evolution of Monopoly Businesses and First Movers\n2024-01-18T22:56:06.495Z\n- **Monopoly**: A situation in which a single company or group owns all or nearly all of the market for a given type of product or service.\n- **First mover**: The first company to enter a new market or introduce a new product.\n- Google and Facebook were not the first in their respective fields but excelled due to their innovation and execution.\n- Facebook was the first social networking site to implement real identities, distinguishing it from earlier concepts of cyber identities.\n\n## Rethinking Medicine and Education\n- The question at the end would benefit from more context or clarification. If you provide additional details, I'd be happy to offer possible recommendations.",
      "status": "inactive"
    },
    {
      "title": "359.4 Forecasting",
      "note_id": "11c50331-b42e-46aa-a08d-0665f63b5bf4",
      "markdown": "# 359.4 Forecasting \n\n## Homework Schedule Discussion \n<2024-01-19T02:04:21.528Z>\n- Thursday is designated for homework  \n- Homework due the Sunday of the following week, allowing for a 10-day timeframe  \n- Overlapping with new homework assignments occurs at times   \n\n## Time Series Data and its Importance\n2024-01-19T02:05:53.538Z\n- Time series: Data that is observed or recorded at regular time intervals.\n- Time series data is crucial in various applications, and understanding its analysis is essential for several fields.\n\n## Introduction to Machine Learning Models\n2024-01-19T02:06:53.677Z\n- Neural networks are powerful tools used widely in real-world applications.\n- There are other models that perform well too, such as ARIMA models, regression models, decision trees, random forests, and gradient boosting.\n\n## Forecasting and Prediction\n2024-01-19T02:10:20.745Z\n- Forecasting is used to project the future values of a variable, often applied to time series data such as sales numbers or sensor data.\n- Prediction involves understanding the relationship between variables and making categorical statements about the future, such as predicting customer behavior or identifying patterns in multivariate time series data.\n\n## Multivariate Time Series\n2024-01-19T02:11:53.003Z\n- Multivariate time series involve multiple variables that vary over time, with interactions between them.\n- Autocorrelation: The correlation of a variable with itself at different time lags.\n\n## Macroeconomic effects and commodity prices\n2024-01-19T02:14:14.622Z\n- The **macro effect** refers to a broad, overall trend that affects the economy at a large scale.\n- Real estate prices can be influenced by macro effects, such as a spike in demand due to external factors like economic conditions or global events.\n- **Commodities** are primary goods that are traded in the market, such as gold, silver, and grain.\n- Commodity prices are related to global macroeconomic events, such as natural disasters or geopolitical conflicts.\n- Attempting to predict commodity prices involves analyzing a variety of information, including market trends, news, and historical data.\n\n## Identifying Patterns in Commodity Markets\n2024-01-19T02:16:04.337Z\n- Car dealerships attacked in the Red Sea by rebels causing disruptions in commodity markets.\n- Ships rerouting around Africa affecting commodity prices.\n- Proposal to use machine learning to identify potential patterns and effects on commodities based on news analysis.\n\n## Auto Regressive Integrated Moving Average (ARIMA) Models\n- ARIMA models are used to model future values as a linear combination of past observations.\n- They work well with univariate time series data.\n- The models assume that the time series is stationary, meaning it does not have any global or seasonal trends.\n\nVocabulary:\n- **ARIMA**: Auto Regressive Integrated Moving Average model, used for time series analysis.\n\n## Auto regressive integrated moving average (ARIMA) models\n2024-01-19T02:17:11.928Z\n- ARIMA models are used to model future values as a linear combination of past observations.\n- They are suitable for univariate time series data.\n- They cannot handle seasonal or cyclical components inherent in the data.\n- Stationarity is a key assumption for ARIMA models.\n\n## Auto Regression and Moving Average Models\n2024-01-19T02:21:58.690Z\n- Auto regression (AR) means regressing on the variable itself.\n- Regression on past values is done to predict the current value, i.e., YT from YT-1, YT-2, etc.\n- Moving Average (MA) involves creating a weighted sum of recent values in the time series for the variables based on its past.\n- The model uses a weighted sum, which is a linear combination of past values and recent values of the errors.\n- The errors are represented as white noise, as in YT - YT-1.\n\n## ARIMA Model Components\n2024-01-19T02:21:58.690Z\n- ARIMA model includes **autoregressive terms** and **moving average terms**. \n  - **Autoregressive terms**: These are the past values used in the model.\n  - **Moving average terms**: These are the errors between the time steps, which are essentially white noise.\n\n## Understanding Stationarity\n2024-01-19T02:24:52.549Z\n- Stationary time series: It means that the time series does not have a trend or seasonality.\n- Derivative: The rate of change of a quantity with respect to its independent variable.\n- Discrete data: Data that is separate and distinct, not continuous.\n- ARIMA model: Autoregressive Integrated Moving Average model, a popular statistical method for time series forecasting.\n\n## Google Stock Price Analysis\n2024-01-19T02:26:49.554Z\n- The Google stock price data over 200 days is analyzed for stationarity and trend.\n- Non-stationary time series: Time series with a trend, where the derivative of the time series is non-zero, indicating a consistent pattern of change over time. \n- Stationary time series: Time series where the observations' differences are taken to remove trends. This helps in creating a constant mean and variance over time.\n- Trend line: A line that depicts the general direction and pattern of change in the data.\n\n## Random Walk Model\n2024-01-19T02:28:57.551Z\n- **Oscillating**: Moving back and forth in a regular rhythm.\n- **Consecutive Observation**: A series of observations made in sequential order.\n- **White Noise**: A sequence of uncorrelated random variables.\n- In economic data, differences between times can exhibit characteristics of white noise.\n- The difference between two times in a random walk model is called white noise.\n\n2024-01-19T02:30:01.552Z\n- **Centigradation**: Not a standard term; may refer to centigrade, a scale of temperature.\n- The discussion introduces the concept of white noise in the context of economic data.\n- White noise is characterized by normally distributed error.\n\n## Barriers of Centigration of Water\n2024-01-19T02:30:01.552Z\n- No specific vocabulary or equations mentioned.\n- No other relevant information provided.\n\n## Auto-Regressive Models and Time Series Generation\n2024-01-19T02:31:33.556Z\n- Auto regressive (AR) models are linear regression models for predicting or modeling a time series at the current time step as a constant plus a linear combination of P previous time steps. \n- AR models are a classic and well-functioning model for time series.\n\n## Time Series and ARP Model\n2024-01-19T02:33:08.553Z\n- Time series: A sequence of data points, measured typically at successive points in time.\n- ARP model: Autoregressive process model, a stochastic process used in time series analysis.\n- The time series was created using an AR model and white noise.\n\n## Time Series and Moving Average Model\n2024-01-19T02:34:34.554Z\n- Time series: A series of data points indexed in time order\n- Moving average model: A statistical model used to analyze time series data and smooth out short-term fluctuations to identify trends\n- White noise errors: Random and uncorrelated errors in a time series\n- ARIMA model: Autoregressive Integrated Moving Average model, a popular statistical method for analyzing and forecasting time series data\n\n## Time Series Models and Seasonal Decomposition\n2024-01-19T02:36:07.559Z\n- Time series models use data as parameters for moving average and autoregressive models.\n- Moving average models use five as the parameter.\n- Autoregressive models generate a time series based on white noise, with error terms being white noise at specific time steps.\n- Removing trends and seasonal effects is important for time series analysis.\n\n## Seasonal Decomposition and Sarma Models\n2024-01-19T02:36:59.555Z\n- Seasonal decomposition is a technique to identify frequencies and cyclical functions in data.\n- Similar to Fourier transforms but specifically for identifying seasonal patterns.\n- Sarma models add seasonal terms to the ARMA (AutoRegressive Moving Average) models.\n- Parameters PDQS are used to model seasonal effects, where \"S\" specifies the seasonal period, for example, in monthly data.\n  \n## Seasonal Terms in Aroma Models\n2024-01-19T02:38:42.557Z\n- Aroma models: statistical models used to capture the fragrant characteristics of products, such as food or beverages.\n- Parameters pdqs: specific parameters used in time series analysis, representing the autoregressive order, differences, seasonal autoregressive order, and the seasonal differences.\n- Stationary data: data with constant mean, variance, and autocorrelation over time.\n\n## Data Collection and Preprocessing\n2024-01-19T02:40:16.556Z\n- Temperature is taken daily and there is seasonal data that varies by quarter.\n- The temperature data spans from 1850 to the present, with one value for every month.\n- Anomalies are deviations from the average temperature.\n- The data can be obtained from the National Center for Environmental Information.\n- The temperature data is represented as floats.\n\n## Monthly Temperature Anomalies Data Set\n2024-01-19T02:42:32.555Z\n- The dataset provided contains temperature anomalies for each month since 1850.\n- Anomalies refer to deviations from the average temperature.\n- The data is available on the National Center for Environmental Information government page.\n\n## Identifying Stationary Time Series\n2024-01-19T02:43:55.556Z\n- The key task at hand is to identify if the time series is stationary.\n- **Stationary**: A stationary time series is one whose statistical properties such as mean, variance, and autocorrelation are constant over time.\n\n## Stationary data and Seasonality\n2024-01-19T02:45:54.555Z\n- Data that doesn't exhibit significant changes in statistical properties over time is known as **stationary data**. \n- **Seasonality** refers to a characteristic of time series data where patterns repeat at regular intervals.\n\n## Installing the Stats Model\n2024-01-19T02:47:48.560Z\n- The remark comes with the stats model package.\n- Define the order as 311, where P=1, d=1, and Q=1.\n- This is an autoregressive model with P=3.\n\n## Time Series Analysis and Forecasting\n2024-01-19T02:49:02.559Z\n- Time series data after differencing to achieve stationarity.\n- The red line represents the forecasted values.\n- The model appears to struggle to capture the seasonality.\n\n## Modeling Time Series Data\n2024-01-19T02:51:27.566Z\n- Modeling parameters include a seasonal effect to account for general seasonality over 12 months.\n- The temperature time series data revealed a global trend of increasing temperature and a seasonal effect.\n- The model accurately captured the trend and seasonality, providing a reasonably accurate forecast.\n\n## Components of ARIMA and SARIMA models\n2024-01-19T02:53:51.567Z\n- ARIMA model and its parameters:\n  - **How long do you look back**: Determines the number of past time steps taken into account for forecasting.\n  - **How much difference is required**: Specifies the difference between the time series data to make it stationary.\n  - **How many error terms to take into account**: Specifies the number of steps back in the error process.\n- SARIMA model and its additional parameters:\n  - **Seasonal pattern identification**: Helps identify global seasonality, such as a 12-month season for temperature data.\n  - **Difference parameter for seasonal pattern**: Determines how many times the seasonal difference should be taken into account.\n\n## Identifying Invitations and Using Stationary Models \n2024-01-19T02:54:40.560Z\n- In the code, the first step is to identify whether an invitation is present or not.\n- If the invitation is stationary, a difference is taken. \n- The model used for Army data didn't block at the time series itself, but instead, all differences were fed into the model.\n- The model in use doesn't require taking differences before feeding the data and then fitting the model. \n- The model is then fitted, followed by forecasting. \n\n## Using Neural Networks for Time Series Data Preprocessing\n2024-01-19T02:57:29.563Z\n- Scale the data to fit within the 0 to 1 range to facilitate learning.\n- The data consists of temperature values.\n- To use a neural network on time series data, the series needs to be converted into sequences for supervised learning.\n\n**Vocabulary:**\n- **Supervised learning algorithm**: A type of machine learning algorithm where the model is trained on input-output pairs, and the aim is to learn a mapping from the input to the output.\n\n**Equations:**\n- YT - 1, YT - 2: Observations or input values.\n- Y_target: The value being predicted or forecasted.\n\n## Input and Output Features in Time Series Data\n2024-01-19T02:58:57.567Z\n- Input features and output are present in time series data as well.\n- The value being predicted or forecasted is defined as the target in time series data.\n- Previous values (YT - 1, YT - 2, etc.) are treated as observations or input for forecasting the next time step.\n\n## Recurrent Neural Networks (RNN)\n2024-01-19T03:00:34.570Z\n- RNNs function by taking input data in sequence and can assign a time component to the input data.\n- Classic neural networks lack a sense of time and cannot handle time series data efficiently.\n  \n## Understanding LSTM and RNN\n2024-01-19T03:02:38.555Z\n- LSTM: A type of recurrent neural network (RNN) that is designed to overcome the vanishing and exploding gradient problems. LSTMs are able to learn long-term dependencies.\n- De-emphasize: To reduce the importance or priority of something.\n\n## Differences between Neural Networks and CNNs\n2024-01-19T03:04:05.552Z\n- Time dimensions are a significant difference between neural networks and convolutional neural networks (CNNs).\n- The former (neural networks) do not have any sense of time and are timeless, while the latter (CNNs) are able to process time-related data, which makes them powerful for tasks involving text, speech, and video processing.\n\n## Challenge to Find Pythonic Ways for Performing Operations on Sequences\n2024-01-19T03:06:20.555Z\n- The user wants to find efficient ways to perform operations on sequences using Python.\n- They emphasize the challenge of implementing these operations in an efficient manner.\n- The functionality should allow stepping back by 24 steps in the sequence.\n\n## Using a Normal Neural Network and Feeding Sequences\n2024-01-19T03:07:34.554Z\n- **Normal neural network (NN)**: A type of neural network where the layers are connected sequentially, without any specialized structure.\n- The concept of using a normal neural network in the context of working with sequences is being discussed.\n\n## Reading in Temperature Plot and using Functional API\n2024-01-19T03:10:15.612Z\n- Reading and scaling temperature plot for San Francisco sequencies.\n- Shape of X is one dimensional.\n- Using the functional API to construct the network.\n- Defining input shape on the first layer and assuming the outputs of the first layer are fed into the second layer.\n\nVocabulary:\n- **Sequential API**: A simple way to construct a neural network by adding layers in a sequence, from input to output.\n\nEquations:\n- Loss function: Mean square error (MSE)\n- Optimizer: Adaptive stochastic gradient descent (Adam)\n\n## Functional API Code explanation\n2024-01-19T03:11:20.927Z\n- Code uses the functional API for neural networks\n- Input shape is defined as (you know, comma)\n- Output layer has only one node for forecasting\n- Loss function used is mean square error\n- Optimizer used is Adaptive stochastic gradient descent (Adam)\n- Training on a sample dataset, not the full dataset\n- Number of parameters in the model is 11,500\n\n## Comparison of Neural Networks and Decision Tree Models\n2024-01-19T03:14:38.561Z\n- The speaker wants to compare different models to understand their effectiveness\n- Mentions that neural networks are not a \"Silver Bullet\" and are good for many applications, but not always the most effective\n- Refers to a paper on forecasting competitions at Walmart where neural networks were not the winning models\n- The winning model used a truncated singular value decomposition to remove signal variations of data\n- Highlights that decision tree models are nonparametric supervised learning algorithms, used for both regression and classification tests\n\n### Discussion of Neural Networks and Decision Trees\n2024-01-19T03:16:19.559Z\n- Neural networks don't always outperform other algorithms.\n- Decision tree models are nonparametric but supervised learning.\n  - Nonparametric: Does not assume a set form for the distribution of the data. \n  - Supervised learning: Learning from labeled training data.\n- Decision trees can be used for both regression and classification tasks.\n- Decision trees involve asking questions based on features to make decisions and constructing a tree.\n\n## Decision Trees and Random Forests\n2024-01-19T03:17:48.557Z\n- Decision trees are a non-parametric algorithm used for both classification and regression tasks.\n- Feature vectors: A feature vector is an n-dimensional vector of numerical features that represent an object. In this case, the feature vectors represent the data used to construct the decision tree.\n- Overfitting: Overfitting occurs when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data.\n\n## Decision Trees and Random Forests for Parameter Significance\n2024-01-19T03:18:43.560Z\n- Decision trees and random forests are useful for identifying important parameters.\n- **Ensemble**: A group of items viewed as a whole rather than individually.\n- Ginny index: Might refer to the Gini index, a measure of statistical dispersion.\n- When using decision trees or random forests for time series, the issue of turning the sequence into a time series needs to be addressed.\n\n## Random Forest and Decision Trees\n2024-01-19T03:19:46.560Z\n- Random Forest and Decision Trees are combinations of multiple decisions into one output.\n- Random Forest is an ensemble of decision trees that feed into each other.\n- When using decision trees or random forests for time series, the issue of sequencing needs to be addressed.\n\n## Decision Tree for Time Series Analysis\n2024-01-19T03:22:37.564Z\n- Decision fees: The fees incurred in making a decision. \n- Trends: Patterns or inclinations in data over time. \n- Decision tree: A decision support tool that uses a tree-like model of decisions and their possible consequences. \n- Time series: A sequence of data points indexed in time order.\n\n## Understanding the Process of Scoring and Evaluation\n2024-01-19T03:22:37.564Z\n- Scalar: A term related to scaling, used to normalize the data and bring all features to a similar scale to prevent any feature from being too dominant. The term \"scalar\" refers to a mathematical operation that transforms the data in a consistent way.\n- Inverse trend: Refers to the reversal of a pattern or trend. In the context of data, it signifies the restoration of the original trend after scaling or transformation.\n- Mean square error: A measure of the average squared difference between the actual and predicted values. It indicates the quality of the model's prediction.\n\n## Mean Square Error (09 vs 10 to the -9) \n- The mean square error (MSE) is a measure of the average of the squares of the errors or deviations. \n- Green mean square error is 09 while the decision tree mean square error is 10 to the -9, indicating a significant improvement in accuracy. \n\n## Root Mean Square Error (RMSE)\n2024-01-19T03:23:55.560Z\n- **Root Mean Square Error (RMSE)**: A measure of the differences between values predicted by a model or an estimator and the values observed. It is used for the evaluation of models in statistics and machine learning.\n- Decision tree RMSE: 10^-9, which equates to 0.90\n- The time series is extremely close to the decision tree's performance.\n\n## Gradient Boosting, Random Forests, and Decision Trees\n2024-01-19T03:27:56.568Z\n- Gradient boosting, random forests, and decision trees are related models\n- They are effective for time series problems\n- An article compares gradient boosting and deep learning for credit score prediction in Financial Services\n- The experiment showed that gradient boosting is more powerful than deep learning and has lower computation requirements\n\n## Discussion of Forecasting Techniques\n2024-01-19T03:29:44.563Z\n- **Computation Requirements**: The amount of computational resources needed for a specific task.\n- Retail Forecasting: Predicting future sales and demand for products in a retail setting.\n- Causal Factors: Variables or events that directly influence the outcome of interest.\n- Unitary Sales Forecasting: Predicting sales for individual units of a product.\n- Ensemble: A combination of multiple forecasting techniques or models.\n- Gradient Boosting Trees: A machine learning technique that builds trees sequentially and corrects the errors of the previous trees.\n- Overfitting: When a model learns the training data too well and fails to generalize to new data.\n- Neural Networks: A type of machine learning model inspired by the structure of the human brain.\n- Vanishing and Exploding Gradients: Issues related to the training of neural networks when the gradients become extremely small or large, causing learning to slow down or fail to converge.\n\n## Vanishing Gradients\n- The vanishing gradients problem occurs when the gradients in a neural network become very small as the network approaches the minimum of the loss function.\n- **Vanishing gradients**: This refers to the situation where the derivatives become extremely small, making it challenging for the network to learn effectively.\n- In a computer, mapping a very small number can lead to rounding problems, and the network may not make significant progress in each time step, which is known as vanishing gradients.\n\n## Discussion of Neural Networks and Decision Trees\n2024-01-19T03:32:24.563Z\n- Decision tree: A decision tree is a flowchart-like tree structure where an internal node represents a feature or attribute, the branch represents a decision rule, and each leaf node represents an outcome. \n- Neural networks: A neural network is a series of algorithms that endeavors to recognize underlying relationships in a set of data through a process that mimics the way the human brain operates.\n- Stock price analysis: Comparing the use of decision trees and neural networks in analyzing stock prices.\n- Sequences of data: Refers to the arrangement of data or events in a successive order.\n\n## Google Stock Price During COVID\n2024-01-19T03:33:26.013Z\n- Time series data: Data points collected or recorded at specific intervals of time.\n- Sequences: A set of data points collected or recorded in a particular order.\n\n## Time Series Analysis and Data Splitting\n2024-01-19T03:34:28.472Z\n- Time series analysis involves analyzing data points collected or recorded at regular intervals of time.\n- In time series analysis, the training and test data sets are created according to the sequence of the data.\n- The training data set consists of the first 70% of the time series, and the test data set consists of the last 30% of the time series.\n- Random sampling for the test data set is not suitable for time series analysis, as it may jumble up the sequence of data points.\n\n## Time Series Data Splitting\n2024-01-19T03:35:22.281Z\n- Time series data is split into two parts - train and test data.\n- TensorFlow is used for data splitting and validation.\n- A validation parameter, \"cold equals 3\" is mentioned, indicating 30% of the data to be tested.\n\n## Data Splitting and Preprocessing\n2024-01-19T03:36:37.575Z\n- The data has been split into training and testing sets for model development.\n- The training data set has a length of 51.\n- The testing data set has been created with the same length as the training data set.\n\n## Creating Test Data Sets for RNs\n2024-01-19T03:37:36.576Z\n- Additional dimension needed for RNs\n  - RN: Recurrent Neural Network\n- Need to add the additional dimension into the data set\n- Scaling using the max value\n- Implementation using an RN and a DNN\n\n## Implementing RNN and CNN Models and Training in Keras\n2024-01-19T03:38:58.575Z\n- Implementing RNN (Recurrent Neural Network) and CNN (Convolutional Neural Network) models in Keras for training.\n- Utilizing units return sequence and several layers of RNN in the first model.\n- Mention of theory behind implementing RN (Recurrent Neural Network) and how it is similar to defining DNA units.\n- Intention to build a CNN model with specifications such as \"128 64\".\n\n## Models Compilation and Training Process\n2024-01-19T03:40:32.578Z\n- Models have a compile statement where the optimizer and loss function are defined.\n- Callback hooks are used for metrics tracking.\n- The model is printed and a summary is created.\n- Different models like LSTM and GRU are being experimented with.\n- SGD (Stochastic Gradient Descent) was used instead of Adam optimizer, and it performed better.\n\n## Training and test data scaling\n2024-01-19T03:42:03.581Z\n- Function used to standardize the input features, ensuring all features have the same scale.\n- **Scalar**: A quantity that has magnitude and direction, often used in the context of vector quantities in physics. \n\n## Model Training and Decision Tree\n2024-01-19T03:43:22.577Z\n- Values are printed before running the decision tree.\n- The decision tree works differently and is not defined as part of the model.\n- A function for making predictions and plotting the results is utilized.\n- Decision tree code had to be duplicated as a solution due to difficulty in integrating it with other parts.\n\n## Model Training Results \n2024-01-19T03:44:32.576Z\n- Currently training the LSTM model with 56,000 parameters. \n- The RNN model has already been trained, with the results indicating the Google stock price. \n- The training data consistency has been addressed, resulting in improved performance. \n- The LSTM model has shown improved performance as compared to the RNN model. \n- The decision tree model also demonstrates good performance, though not as fantastic as before. \n\n## Model Comparison and Training Results\n2024-01-19T03:45:24.576Z\n- The models compared are LSTM, decision tree, neural networks, GRU, and R&M.\n- The decision tree performs similarly to LSTM and neural networks.\n- The LSTM model is visualized as the yellow curve.\n- The GRU and R&M models show similar results.\n\n### Time Series Analysis and Machine Learning Models\n2024-01-19T03:46:53.579Z\n- **Models and Loss of Accuracy:**\n  - Four models were trained, and the loss of accuracy for each was observed.\n  - Mention of \"meat update\" - more context needed to interpret this phrase.\n  - The best-performing model was identified as the GRU (Gated Recurrent Unit) model.\n  - Trained models were not large, with approximately 56,000 parameters.\n  - It was mentioned that bigger models could potentially improve performance, but previous attempts with larger architectures did not yield significantly better results.\n  - Time series analysis was described as quite challenging.\n\n## Lecture Overview and Classic Algorithms for Time Series Analysis\n2024-01-19T03:48:40.511Z\n- Classic algorithms for time series analysis included decision trees, random forests, and gradient boosting.\n- Supervised learning algorithms like neural networks require time series data to be converted into sequences for input.\n- Neural networks are structured to expect data in sequences, making them effective for time series analysis and tasks associated with time dimensions such as text understanding.\n- The homework assignment is to train a deep neural network for predicting taxi fares in New York.\n\n\n## Homework Assignment Details\n2024-01-19T03:49:43.238Z\n- The homework involves predicting taxi fares using a deep neural network on the provided dataset.\n- The dataset is available on Blackboard and the task is to predict the fare amount for a ride in New York City.",
      "status": "inactive"
    },
    {
      "title": "laekjgheal;kdfgjladkj",
      "note_id": "d16d45c5-1e07-40dc-a754-b82177f05080",
      "markdown": "# laekjgheal;kdfgjladkj",
      "status": "active"
    }
  ]
}