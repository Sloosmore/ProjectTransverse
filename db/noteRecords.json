{
  "noteRecords": [
    {
      "title": "intro to python",
      "note_id": "c5beeea6-4b22-40c5-a6bb-5b2f19132a59",
      "markdown": "# intro to python\n\n## Overloaded Operators\n<2024-01-15T04:41:49.172Z> \n- Operators can be overloaded, allowing them to behave differently based on their context.\n- Example: combining a number with a string using multiplication will result in a new string with the specified number of replicas.\n- Example: two strings can be combined using the addition operator.\n\n## Static Semantics\n<2024-01-15T05:11:23.224Z>\nIn the context of programming, static semantics is highlighted as an important factor. It involves the analysis of a program's text to determine its meaning, even before it is run.\n\n\n## Type Checking in Python\n<2024-01-15T05:12:24.228Z>\n- Python performs type checking to detect data type-related errors.\n- Inconsistencies in type checking can lead to issues downstream.\n- Different programming languages exhibit a spectrum of type checking, ranging from weak to strong typing.\n\n<2024-01-15T05:13:16.223Z>\nThe lecturer highlights the differences in type checking in Python compared to other languages, especially those with weak typing.\n\n<2024-01-15T05:14:19.224Z>\nThe lecturer delves into an example to demonstrate Python's type checking behavior using the \"less than\" operator, where unexpected comparison outcomes occur between strings and numbers.\n\n<2024-01-15T05:15:19.227Z>\nThe lecturer emphasizes the importance of exercising type discipline in coding to ensure the correct usage of operators and operands.\n\n<2024-01-15T05:17:02.223Z>\nFurther discussion involves the unconventional meanings of certain operators in Python, such as division and the order of operations for arithmetic expressions.\n\n<2024-01-15T05:18:10.218Z>\nThe lecturer continues the discussion about the order of operations, emphasizing the need to be explicit when specifying the desired order of evaluation in expressions.",
      "status": "inactive"
    },
    {
      "title": "MIT lecture 3",
      "note_id": "7dfafac7-faec-4c06-987f-40e4f4e1cee6",
      "markdown": "# MIT lecture 3\n\n## Programming Basics\n<2024-01-15T05:19:32.217Z> \nThe lecturer begins by summarizing the content covered in the previous lectures. They outline the basic elements of programming, including data (numbers, strings, and booleans), operations (such as addition and multiplication applicable to numbers and strings), and commands/statements for changing the flow of control. They also introduced loop mechanisms and emphasized the power of these fundamental instructions for writing common patterns of code.\n\n## Importance of Good Programming Style\n<2024-01-15T05:23:13.783Z>\nThe lecturer highlighted the significance of good programming style, emphasizing the following key points:\n- Use of comments for code clarity and debugging.\n- Type discipline to ensure expected types for operations.\n- Descriptive and meaningful variable names.\n- Testing all possible branches within the code.\nThe focus was on promoting good programming practices to write high-quality code.\n\n\n## Iterative Programs\n<2024-01-15T05:24:32.240Z>\nThe lecture concluded with a hint at building upon the basics discussed previously. The lecturer indicated the intention to delve into iterative programs, focusing on patterns of code that tackle certain classes of problems and tools to aid in understanding those pieces of code. Emphasis was placed on the selection of variables for counting within a process.\n\n<2024-01-15T05:25:24.748Z>\nThe lecturer outlined the key steps for creating an iterative program: Initializing the variable, setting up the right end test to determine loop completion, ensuring the block of code inside the loop contains instructions that change the variable being counted, and deciding what to do when the loop is done.\n\n<2024-01-15T05:26:25.548Z>\nA reminder was given that the block of code in an iterative program represents the set of instructions to be executed each time through the loop and highlighted the necessity of changing the counting variable within the block of code to avoid infinite loops. The importance of having a clear structure when mapping a problem into an iterative program was emphasized.\n\nThis section gave a high-level overview of the considerations and steps involved in creating iterative programs, with a focus on the manipulation of variables within the iterative process.",
      "status": "inactive"
    },
    {
      "title": "ML3",
      "note_id": "f9c649d2-8423-4368-a41a-e7ee7f24e65b",
      "markdown": "# ML3 \n\n## Machine Learning Review\n2024-01-17T01:46:35.872Z\n- Supervised learning involves feature extraction and learning algorithms\n- Feature extraction converts examples into a feature vector\n- Linear predictors or neural networks provide scores for the feature vectors\n- Scores are defined via simple or more complex linear combinations\n- Score functions can be used for classification and regression\n\n## Back Propagation Example and K Nearest Neighbors\n2024-01-17T01:46:35.872Z\n- Back propagation exemplifies how neural networks update weights during training.\n- K Nearest Neighbors is an algorithm used for classification and regression based on the proximity of data points.\n\n## True Objective of Machine Learning\n2024-01-17T01:47:35.473Z\n- Minimizing error on unseen future examples\n- Learning about machines\n- **Machine Learning Vocabulary**\n    - *Gradient*: A vector that points in the direction of the greatest rate of increase of a function and whose magnitude is the slope of the function in that direction.\n- The true objective is minimizing error on unseen future examples, rather than just minimizing error on the training set.\n- Unsupervised learning will be discussed later.\n\n\n## Training Loss as an Objective Function\n2024-01-17T01:51:21.884Z\n- Switching gears to talk about unsupervised learning.\n- Questioning whether training loss is a good objective function.\n- The described algorithm suggests outputting the weight seen in the training set, indicating a need for objective functions beyond simply minimizing training loss.",
      "status": "inactive"
    },
    {
      "title": "ITP 359 lecture 3 Review of supervised ML",
      "note_id": "1773f188-fafe-463e-861d-56fea9219a4f",
      "markdown": "# ITP 359 lecture 3 Review of supervised ML\n\n## Troubleshooting Testing (2024-01-17T02:00:52.619Z)\n- Testing was done before arrival but encountered an error.\n- Describing need for uninterrupted functioning and optimization.\n- Looking to address refreshing issue when using effects and location hook.\n- Working on proper re-rendering of the system.\n- Referring to seller's rooms in the discussion.\n\n## Testing and Error Handling\n- Testing the functionality before the official use to identify and resolve any potential errors.\n- In this context, \"error\" refers to an unexpected problem or malfunction in the system.\n\n## Testing and Debugging\n2024-01-17T02:00:52.619Z\n- Testing a feature before it's ready\n- Encountering errors while testing\n- Needing to optimize the feature\n- Difficulty with refreshing and re-rendering when using location hook to pull data from a link\n\n## Review of Last Semester - 2024-01-17T02:02:50.993Z\n- Next week's session will focus on forecasting.\n- The upcoming topics include Markov chains and naive Bayes outboard.\n- These concepts are important statistical methods.\n\n## Sequential Data in Finance and Audio Processing\n2024-01-17T02:04:42.744Z\n- Algorithmic traders often use sequential data for trading, which is also utilized in audio processing.\n- Hidden Markov models, autoencoders, and variational autoencoders are popular in processing sequential data for various applications.\n\n## AI Approaches and Modeling Techniques\n2024-01-17T02:06:10.989Z\n- Generated AI mentions the use of generators in AI models\n- Plans to start with variation oil and odors in those types of models\n- Homework on forecasting to be completed within two weeks\n- The first homework may be released on Thursday or Monday/Tuesday\n- Emphasis on not rushing through the material and having ample time for learning\n\n## Approximation Problem and AI Approaches\n2024-01-17T02:07:39.014Z\n- **Approximation Problem:** A problem involving finding a function that approximates a given target, often tackled using statistical or numerical methods.\n- AI approaches primarily involve dealing with approximation problems.\n- Techniques for addressing approximation problems include statistical methods, numerical methods, linear regression, logistic regression, and neural networks.\n\n## Regression and Classification\n2024-01-17T02:09:09.171Z\n- Neural network used for classification and regression\n- K-Nearest Neighbor algorithm used for classification and recommendation systems\n- Brief intro to Association Analysis and Principal Component Analysis (PCAs)\n\n## Traveling Salesman Problems and Research Competition\n2024-01-17T02:10:14.064Z\n- Traveling Salesman Problems involve finding the shortest possible route that visits each city exactly once and returns to the original city\n- Research competition likely involved exploring algorithms and strategies for solving complex problems like the Traveling Salesman Problem\n- Statistical standpoint refers to analyzing data from a mathematical perspective, considering patterns and probability\n- Machine learning standpoint involves using algorithms and statistical models for computers",
      "status": "inactive"
    },
    {
      "title": "ITP 359 lecture 4 Neural Networks",
      "note_id": "856f1b5f-39b5-48c9-a549-b526b76ef6bc",
      "markdown": "# ITP 359 lecture 4 Neural Networks \n\n## Neural Networks and Approximators <2024-01-17T03:15:13.293Z>\n- Neural networks as approximators - The neural network is used as an approximator by approximating a function with the help of learning from training examples.\n- Basic idea of neural networks - Each node in the network uses a linear combination of feature vectors and associated weights.\n- Vocabulary: \n  - **Approximator**: In machine learning and statistics, an approximator is a method or learning algorithm that seeks to approximate a target function through training on example inputs, and adjusting the model's parameters to minimize the difference between the predicted and actual outputs.\n\n## Understanding the Perceptron Training Process\n2024-01-17T03:20:32.067Z\n- Perceptron training minimizes misclassifications\n- Misclassifications are minimized using absolute value\n- Weight updating is based on data points and their associated calculations\n- Learning rate multiplied by the distance between the approximation and the actual value\n\n## Training a model\n2024-01-17T03:21:48.840Z\n- Start with the weights being small, not too big to avoid issues with training.\n- Perform steps for each data point, including calculating the linear combination, applying an activation function, and updating the weights.\n\n## Perceptron Training Process \n2024-01-17T03:22:59.749Z\n- Updating weights in each step for each data point\n- Calculating the error and updating weights until a very small error is achieved\n\n## Linearly Classifiable Data and Neural Networks\n2024-01-17T03:25:44.088Z\n- Linearly classifiable data: Data that can be separated into different classes using a straight line or hyperplane\n- Neural networks: A series of algorithms that endeavors to recognize underlying relationships in a set of data through a process that mimics the way the human brain operates\n\n## New AI Architecture\n2024-01-17T03:27:15.177Z\n- The new AI architecture on Netflix involves a dual network with multiple layers. \n- It utilizes activation functions such as sigmoid function, real or logistics function, and hyperbolic tan function to replace the step function.\n- The architecture emphasizes the use of non-linearities in constructing multilayer neural networks.\n\n## Neural Network Definition\n2024-01-17T03:29:09.085Z\n- The network is defined as starting at the end and working backward, with the output being an activation function of the output weights times the output of the final layer. \n- Each layer is defined by a number of \"Lambda\" nodes, with all nodes connected to each other, creating a fully connected feedforward network.\n\n## Neural Network Structure\n2024-01-17T03:32:14.821Z\n- The output of a neural network depends non-linearly on the last layer, and this layer depends on the previous layer and so on, creating a recursive structure within the network.\n- The network can be visualized as an onion, starting with the output and then working back to the input, with calculations being made as it progresses backward through the layers.\n\n## Universal Approximation Theorem\n2024-01-17T03:33:17.509Z\n- The universal approximation theorem, first articulated in 1989, states that a feedforward neural network with a single hidden layer containing a finite number of neurons can approximate any continuous function on compact subsets.\n- Compact subsets are subsets that include their boundary and do not have to satisfy specific constraints on their nature.\n\n## Multi-class Classification and Softmax\n2024-01-17T03:35:46.043Z\n- The raw output of the network is passed through a sigmoid function, mapping the values to a range between 0 and 1, to obtain the probability for each class in a multi-class classification.\n- The issue with using the sigmoid function alone is that the probabilities assigned to each class do not necessarily add up to one, hence creating ambiguity in classification.\n- This method is referred to as multi-class classification, where multiple classes can be associated with the input.\n\n## Sigmoid and Softmax Functions\n2024-01-17T03:37:29.206Z\n- Sigmoid function used in final layer for multi-class classification\n    - Sigmoid function does not ensure probabilities add up to one\n    - Allows for classification into multiple classes\n- Softmax function exaggerates differences in probabilities and ensures sum of probabilities equals one\n    - Enables exclusive belonging or mapping to a class\n\n## Softmax and Multi-Class Classification\n2024-01-17T03:38:30.789Z\n- Softmax ensures that the output values add up to one, which is essential in binary classification to obtain a classification probability.\n- In multi-class classification, you may want to compare probabilities between different outcomes, such as distinguishing between various objects in an image.\n- Softmax exaggerates the distances between classes, making them not directly comparable, which might not be suitable if you want to rank the different classes based on their probabilities.\n\n## Softmax and Cross Entropy Loss\n2024-01-17T03:40:46.085Z\n- Softmax: A function that takes as input a vector of K real numbers, and normalizes it into a probability distribution consisting of K probabilities.\n- Softmax is used in neural networks to provide probabilities for the different classes in a classification problem.\n- Cross Entropy Loss: A loss function used in neural networks when the output is a probability value. It measures the performance of a classification model whose output is a probability value between 0 and 1. \n\n## Cross Entropy Loss\n2024-01-17T03:42:03.824Z\n- Cross entropy loss is very similar to the negative log likelihood and is used in models.\n- In the case of two classes, cross entropy loss becomes the same as the negative log likelihood equation.\n- The derivation of cross entropy loss comes from taking the logarithm and maximizing the likelihood function.\n\n\n## Softmax Activation Function\n2024-01-17T03:43:11.596Z\n- The softmax activation function is used as the final activation function, providing output probabilities that add up to one.\n- It helps in determining a clear winner among the classes in the model.",
      "status": "inactive"
    },
    {
      "title": "Descriptive Data Mining",
      "note_id": "d2ff3d8b-92dc-45ca-8152-81736e53a2f6",
      "markdown": "# Descriptive Data Mining \n\n## Data Mining\n<2024-01-17T17:08:45.764Z> \n- Data mining is the process of analyzing large sets of data to discover patterns, trends, and relationships.\n- With the advancement of technology, we now have the capability to extract valuable insights from the vast amount of data that is being generated.\n\n## Data Mining\n2024-01-17T17:11:47.830Z\n- Data mining is the process of analyzing large sets of data to discover patterns, trends, and relationships.\n- It involves extracting useful information from a large amount of data, often using automated methods or algorithms.\n- Companies use data mining to gain insights into customer behavior, market trends, and other business-related information.\n\n## Interest Rate Period\n2024-01-17T17:11:47.830Z\n\n- Interest rate period is the duration for which a specific interest rate is applicable for a financial product.\n- In AI, data is highly valuable for companies to derive insights and make informed decisions. \n- The process of mining and analyzing data allows companies to extract actionable information and drive value. \n\n## Algorithm and Qualitative Assessments\n2024-01-17T17:13:02.993Z\n- Algorithm: A set of rules or processes to be followed in calculations or problem-solving operations, especially by a computer.\n- Qualitative assessments: Evaluations that are based on qualities or characteristics rather than on quantity or numerical data.\n- Consider the use of qualitative assessments in decision-making processes.\n- Discuss the potential impact of qualitative assessments on business decisions.\n\n## Cluster Analysis and Similarity\n2024-01-17T17:14:11.304Z\n- **Cluster Analysis**: A method of grouping objects based on the similarity of their attributes or characteristics.\n- Data preparation is underway.\n- Two ways of dealing with daily money have been mentioned.\n- Mention of clustering of similarity for computer reports and association rules.\n\n## Computer Grouping and Data Analysis\n- The process of grouping data for significance and analysis.\n- Utilizing computer algorithms to group similar data together for analysis and decision-making.\n- Methods for determining similarity and measuring distance between data points.\n\n## Grouping data and measuring similarity \n- Date-time: 2024-01-17T17:15:36.708Z\n- The data is grouped to find significance and aid in decision-making processes.\n- The computer can identify similarities between data using numerical comparisons.\n- Similarity can be measured by comparing numbers, such as age and income, using methods like z-scores and standard deviation.\n\n## Scale and Range of Values\n2024-01-17T17:17:59.707Z\n- Range of values: The difference between the minimum and maximum values within a dataset.\n- Age versus income scale: Income is in the tens of millions while age has a lesser range.\n- Z-scores: Standardized scores that measure the distance of a data point from the mean in terms of standard deviations.\n- Using z-scores to fix scale issue and bring values to the same scale for comparison.\n\n## Z-Scores Explained\n2024-01-17T17:19:44.396Z\n- Z-Score: A measurement of a value's relationship to the mean of a group of values, measured in terms of standard deviations. It indicates how many standard deviations a data point is from the mean.\n- Discussion of using Z-scores to analyze income data and how it can affect the distance equation in analysis.\n\n## Analysis of Distance and Z-scores\n- 2024-01-17T17:20:55.708Z\n- The computation of distance between data points is essential in analysis\n- Z scores dominate the distance equation, which can skew the analysis\n- Z scores transform data values to represent their distance from the mean\n\n## Standard Deviation and Z-Score\n2024-01-17T17:22:09.712Z\n- **Standard deviation**: A measure of the amount of variation or dispersion of a set of values. It illustrates how much individual data points differ from the mean.\n- **Z-Score**: A statistical measurement that describes a value's relationship to the mean of a group of values and quantifies how many standard deviations above or below the mean a data point is.\n- The discussion pertains to using Z-score values to determine the distance between data points and how it relates to clustering analysis.\n\n## Floating Point Numbers and Categorical Data\n2024-01-17T17:23:25.406Z\n- **Floating point numbers** are numbers that contain a decimal point. They can represent a wide range of values.\n- **Categorical data** refers to data that represents categories or groups, such as colors or types of fruit.\n\n## K-means Clustering\n2024-01-17T17:24:45.638Z\n- **K-means**: A popular unsupervised machine learning algorithm used for clustering data points into a pre-defined number of clusters.\n- Comparing data points to find clusters for poster creation, based on the nearest neighbors.\n- Exploring the process of creating clustered groups on the ground.\n\n## K Means Clustering\n2024-01-17T17:25:41.736Z\n- K Means Clustering: a type of unsupervised learning algorithm used to group similar data points into a fixed number (k) of clusters.\n- The algorithm iteratively assigns each data point to the nearest of k centroids and then recalculates the centroids based on the mean of the assigned data points.\n- Involves choosing the number of clusters, initializing centroids, assigning data points to the nearest centroid, and updating centroids.\n\n## Hierarchical Clustering\n2024-01-17T17:26:51.716Z\n- **Hierarchical clustering**: A method of cluster analysis which seeks to build a hierarchy of clusters. \n- Hierarchical clustering can be advantageous when working with small datasets, as it enables the formation of clusters from individual observations, but it may not be suitable for large datasets as it can become computationally intensive.\n- Outliers can greatly impact the output of hierarchical clustering. The presence of outliers can affect the distance measurements and ultimately influence the clustering process.\n\n## Hierarchical Clustering and Data Preprocessing\n2024-01-17T17:28:04.717Z\n- Hierarchical clustering: A method of cluster analysis that builds a hierarchy of clusters by either merging or splitting them successively.\n- Outliers removal: The process of eliminating data points that significantly differ from the rest of the data to improve analysis accuracy.\n- Handling large datasets: Hierarchical clustering may not be efficient for datasets with thousands or millions of data points.\n\n## Data Mining and Clustering Analysis\n- Data mining is the process of analyzing large data sets to identify patterns and relationships.\n- Cluster analysis is a technique used to group similar data points together based on certain characteristics.\n\n## Data Mining and Cluster Analysis\n2024-01-17T17:31:28.691Z\n- Data mining is the process of analyzing large sets of data to discover patterns or relationships.\n- Cluster analysis is a technique used to group similar items or data points together.\n\n## Converting Language to Numbers\n2024-01-17T17:40:33.694Z\n- **Unstructured Data**: Data that isn't organized in a pre-defined manner and doesn't have a specific data model. \n- Computer processing text requires converting it into structured numerical data for analysis.\n- The process involves identifying important information from the unstructured data to create a set of numbers for analysis.\n\n## Structuring Text Data for Analysis\n2024-01-17T17:44:11.693Z\n- We need to structure the text data to be something that can be plugged into SPSS, Rosen Collins, or equivalent tools.\n- We can create a term document matrix where important words are picked out and presented to the computer as a set of words.\n- A term document matrix allows us to pick out important words and provide this information to the computer, enabling analysis.\n\n## Structuring the Data for Analysis\n2024-01-17T17:44:11.693Z\n- Term Document Matrix: A way to represent the words in a document as rows and the documents themselves as columns. \n- SPSS and Rosen Collins: Refers to statistical analysis software used for processing survey data.\n- The speaker is discussing the process of creating a term document matrix to analyze survey responses.\n\n## Clustering of Textual Data\n2024-01-17T17:45:25.390Z \n- The process involves analyzing the frequency of words in a set of data to identify patterns and similarities.\n- Tokenization is the process of breaking text into individual words or tokens for analysis and is a crucial step in clustering textual data. \n\n## Tokenization and Text Preprocessing\n2024-01-17T17:46:50.004Z\n- **Tokenization**: Process of breaking text into individual words or tokens for analysis.\n- Remove non-word characters and punctuation marks from text data.\n- Convert all words to lowercase to ensure consistent analysis.\n- Identify and handle word endings to unify related terms.\n\n## Identifying and Processing Words\n- **Stacking**: The act of placing one item on top of another.\n- **Algorithm**: A step-by-step procedure for solving a problem or accomplishing a task, often used in computer programming and artificial intelligence.\n\n## Discussion on Natural Language Processing\n2024-01-17T17:48:05.422Z\n- Natural Language Processing (NLP) - a field of AI focused on enabling computers to understand, interpret, and manipulate human language.\n- Importance of standardizing text - converting all text to lowercase can help in simplifying the processing of data and reduce the variations due to upper/lower case.\n\n\n## Data Preprocessing and Text Analysis\n2024-01-17T17:49:07.485Z\n- Word Stemming - the process of reducing words to their root forms, such as converting \"stacking\" to \"stack\" and \"stacked\" to \"stack,\" to facilitate analysis.\n- Handling synonyms - discussing the transformation of similar words to a common representation, e.g., \"courteous\" and \"cordial\" both being transformed to \"polite,\" for more consistent analysis.\n- Removal of rare words - considering the insignificance of words occurring very infrequently, also referred to as outliers, to ensure meaningful and relevant data analysis.\n\n## Creation of Frequency Matrix\n2024-01-17T17:50:10.660Z\n- Creation of a frequency matrix - a tabular representation showing the occurrences of specific words (e.g., \"great,\" \"terrible\") in the dataset, aiding in data summarization and analysis.\n- Decisions based on word frequencies - using the frequency data to make decisions, such as understanding trends, progress, and changes over time based on the occurrences of certain terms.\n\n## Utilizing data for decision making\n2024-01-17T17:51:03.688Z\n- Decision-making based on data - leveraging the processed data to make informed decisions, possibly on a regular basis, to assess progress, trends, and changes in specific terms and overall data analysis.",
      "status": "inactive"
    },
    {
      "title": "Lecture 1: Introduction to Superposition",
      "note_id": "5f50045b-1c0a-4be9-bbff-dc92bc6c86be",
      "markdown": "# Lecture 1: Introduction to Superposition \n\n## Introduction to the Lecture\n2024-01-18T17:42:50.630Z\n- Alan Adams is an assistant professor in course eight, specializing in string theory and its applications to gravity and condensed matter physics.\n- He is excited to teach the course and mentions that quantum mechanics is his daily language.\n\n## Introduction to the Course\n2024-01-18T17:42:50.630Z\n- Alan Adams, assistant professor in course eight, will be teaching the course on string theory and its applications to gravity and condensed matter physics.\n- The course will heavily focus on quantum mechanics.\n\n## Lecture notes, homeworks, exams on Stellar website\n2024-01-18T17:43:49.027Z\n- All course materials, including lecture notes, homework, and exams, will be accessible through the Stellar website.\n  \n## Quantum Mechanics\n2024-01-18T17:44:55.031Z\n- Quantum mechanics is a branch of physics that focuses on the behavior of particles at very small scales, such as the atomic and subatomic levels.\n\n## Course Structure and Expectations\n2024-01-18T17:45:49.027Z\n- The course will entail weekly problem sets, with the possibility of dropping one to account for unanticipated events.\n- Collaboration with other students is encouraged, including working together to solve problems.\n- There will be two midterms, with dates to be announced, as well as a final exam.\n- Clickers will be used and required for participation, although attendance will not be taken.\n\n\n## Utilization of Clickers\n2024-01-18T17:46:34.025Z\n- The organization of the course is based on empirical lessons on effective teaching methods.\n- Clickers will be used as part of the instruction, and students are required to have and register them on the tsg website by the next week.\n- **Clickers**: A classroom response system that allows students to respond to questions electronically.\n- **Empirical**: Based on evidence from observation or experience rather than theory.",
      "status": "inactive"
    },
    {
      "title": "Lecture 2: Experimental Facts of Life",
      "note_id": "c83f880d-7975-45e6-9c75-144754cd06cb",
      "markdown": "# Lecture 2: Experimental Facts of Life \n\n\n## Basic Experimental Facts in Physics\n<2024-01-18T19:20:19.476Z>\n- Physics provides models to understand the universe, rather than abstract truths.\n- Adams exist, supported by various arguments.\n- Randomness is definitely present in the world.\n- The photoelectric effect is structured and observable.\n\nDo you need any definitions of AI vocabulary or additional information on any specific topic related to physics?",
      "status": "inactive"
    },
    {
      "title": "Competition is for Losers with Peter Thiel",
      "note_id": "2979733f-ad5a-4810-9479-edc65b7e4f03",
      "markdown": "# Competition is for Losers with Peter Thiel \n\n## Creating Value in Business\n<2024-01-18T22:14:14.470Z>\n- The basic idea of creating value in business is essential for understanding the foundation of businesses.\n- Creating value in business involves two key elements:\n  - **Creating Something of Value**: Generating a product or service that has worth and usefulness.\n  - **Capturing Some Fraction of Value**: Securing a portion of the value created to make the business profitable.\n- An illustration of this concept can be seen in comparing the US airline industry with a company like Google in the context of size and revenue generation.\n\n## Creating Value and Capturing Value\n2024-01-18T22:14:14.470Z\n- Value creation and capture are fundamental to the success of a business\n- Value creation: Generating something of worth\n- Value capture: Obtaining a portion of the created value\n- Example: Comparison of US airline industry and Google's search business to demonstrate the importance of both value creation and capture\n\n## Air Travel vs Search Engines\n2024-01-18T22:15:16.468Z\n- Is air travel more important than the use of search engines?\n    - Domestic numbers suggest air travel is more significant.\n- Globally, airlines have a much larger market capitalization than search engines like Google.\n- The combined market capitalization of the US airline industry is approximately a quarter of that of Google.\n\n## Perfect competition and monopoly\n2024-01-18T22:16:30.466Z\n- Perfect competition: a market structure in which a large number of firms all produce the same product and no single seller can influence the market price.\n- Monopoly: a market structure in which a single firm is the sole producer or seller of a product.\n\n## Understanding Business Lies\n2024-01-18T22:18:03.470Z\n- Businesses lie about their competitiveness.\n- Spectrum of companies exist from perfect competition to Monopoly.\n- Monopolies pretend to be incredibly competitive to avoid government regulation.\n- Monopolies pretend not to be monopolies to avoid being targeted.\n\n## Understanding Monopolies and Competitive Markets\n2024-01-18T22:19:42.486Z\n- **Monopoly**: A market structure characterized by a single seller and no close substitutes, giving the monopolist significant control over the price of the product.\n- Differentiation is important to track capital and compete effectively.\n- Example: A monopolist describes their business as the union of vastly different markets, making their market seem much bigger, while a non-monopoly describes it as the intersection, making their market seem super small with less competition.\n\n## Understanding Perfect Competition\n2024-01-18T22:21:10.676Z\n- In perfect competition, all market participants have the same level of information and resources, leading to an equal opportunity for success.\n- Capital accumulation in the context of perfect competition refers to the acquisition of financial and physical assets for future profit.\n\n## Buzzword Sharing Mobile Social Apps\n2024-01-18T22:23:19.473Z\n- Buzzword: A term that becomes popular in a specific industry or field for a period of time.\n- Mobile social apps are applications that allow users to interact and share content with others on their mobile devices.\n\n## Technology Market Overview\n2024-01-18T22:24:34.730Z\n- The technology market is a significant, approximately 1 trillion dollar market\n- Google competes with various companies in different segments, such as car companies with self-driving cars, Apple on TVs and iPhones, and Amazon on cloud services\n- There is a concern about potential regulations due to the market's size and Google's competitive presence\n\n## Tech Industry and Monopoly-like Businesses\n2024-01-18T22:25:29.293Z\n- Tech industry in the US has been financially successful due to the creation of monopoly-like businesses.\n- Startups are advised to target small markets to achieve eventual monopoly status.\n- Companies expand their market presence in concentric circles over time.\n- Going after a giant market from day one is a common mistake as it indicates incorrect category definition and excessive competition.\n\n## Building a Monopoly\n2024-01-18T22:26:47.514Z\n- Monopoly: A market structure where a single company dominates the market and has significant market power.\n- Starting a new company with the goal of achieving monopoly.\n- Starting with a small market and gradually expanding in concentric circles.\n- Avoiding going after a giant market on day one to minimize competition.\n- Examples of successful companies like Amazon, eBay, and Facebook starting with small markets and growing over time.\n\n## PayPal Version of Start-up Strategy\n2024-01-18T22:28:10.479Z\n- Started with power sellers on eBay, around 20,000 people\n- Initially faced criticism for terrible customer service\n- Gained brand recognition after two or three months\n- Emphasizes the potential of underrated small markets\n- Small markets can pave the way for scalable business growth\n\n## Views on Market Strategy and Monopolies\n2024-01-18T22:30:23.474Z\n- In the energy market, being a small player among huge competitors can be challenging.\n- Advantages of being the only company in a small ecosystem.\n- Small markets that may seem insignificant at first could hold potential for expansion.\n\n## Reflections on Technology History\n2024-01-18T22:31:14.433Z\n- The history of technology often dictates unique business opportunities.\n- Emphasizes that the next groundbreaking technology will not necessarily replicate previous successes, but rather build on them uniquely.\n\n## AI Vocabulary\n- **Monopoly**: a market structure in which a single seller sells a unique product and there are barriers to entry for other potential competitors.\n- **Network effects**: the phenomenon where a product or service gains additional value as more people use it.\n- **Branding**: the process of creating a unique name and image for a product in the consumers' mind, typically through advertising campaigns and consistent messaging.\n\n## Happy Companies and Unhappy Companies\n2024-01-18T22:32:23.480Z\n- **Monopoly Technology Company**: A company with exclusive control over a product or service in a particular market.\n- Happy companies are unique and doing something very different.\n- Unhappy companies fail to escape the essential characteristic of a monopoly technology company.\n- The speed of transactions on platforms like eBay and PayPal is crucial for business success.\n\n## Monopoly power and discounted cash flow analysis\n2024-01-18T22:35:29.481Z\n- Monopoly power in business\n    - Refers to a situation where a company has exclusive control over a particular market or industry, allowing them to set prices and control supply\n- Discounted cash flow analysis\n    - A method used to evaluate the attractiveness of an investment opportunity by estimating the future cash flows and discounting them back to the present value\n\n## PayPal's Business Evaluation in 2001\n2024-01-18T22:36:40.480Z\n\n- PayPal was in business for about 27 months, experiencing a growth rate of 100% per year.\n- Discounted future cash flows by about 30%.\n- Three quarters of the value of the business in 2001 came from \"beyond.\"\n- Durability of growth is a measurable factor in tech companies' valuation.\n  \n## Value equation and Monopoly characteristics\n- The value equation refers to the qualitative assessment of value in a business context.\n- Monopoly characteristics may include proprietary technology, network effects, and economies of scale.\n- Proprietary technology refers to technology that is owned and controlled by a company, giving it a competitive advantage.\n- Network effects occur when the value of a product or service increases as more people use it.\n- Economies of scale refer to the cost advantages that a business obtains due to expansion.\n\n2024-01-18T22:38:56.484Z\n\n## Chess Strategy and Innovation\n2024-01-18T22:40:07.492Z\n- White advantage in chess: The player using white pieces has a slight advantage at the beginning of the game, about 1/3 of a pawn.\n- Last mover advantage: In chess, there is a concept called \"last mover advantage\" which suggests that the player who moves last has a better chance of winning.\n\n## Technological Progress and Innovation\n2024-01-18T22:40:51.490Z\n- Remarkable progress from steam engine to telephones to computer revolution and more.\n- Technological innovation has impacted various domains, including household appliances and aviation.\n\n## Lack of Rewards for Inventors and Innovators\n2024-01-18T22:42:21.486Z\n- The history of science shows a pattern of inventors not being adequately rewarded for their innovations\n- Not capturing the value of their inventions\n- Scientists and techn innovators not making money despite their groundbreaking work\n\n## Competition in Industrial History\n2024-01-18T22:43:52.491Z\n- The structure of industries and competition has been important for success.\n- In the 1850s, wealth in Britain was primarily held by the landed aristocracy.\n- Only two broad categories exist in the entire history of the last 250 years.\n\n## Coordination\n2024-01-18T22:45:37.495Z\n- Coordination: The act of organizing or integrating different components or processes in a way that they work together efficiently.\n- Integrating different pieces together and doing it in a way that is vertically integrated is valuable in business.\n- It is capital intensive and can be challenging to get people to buy into anything that is super complicated and takes a long time to build.\n- Tesla's approach involved integrating card distributors to prevent them from taking all the money.\n- Vertical integration is an underexplored modality of technological progress.\n\n## Vertical integration in aerospace industry\n2024-01-18T22:47:21.067Z\n- Large aerospace companies have single source subcontractors\n- These subcontractors charge monopoly profits, making it difficult for aerospace companies to make money\n- Vertical integration is an underexplored modality of technological progress\n\n## Rationalization\n2024-01-18T22:49:33.490Z\n- **Rationalization**: the action of attempting to explain or justify behavior or an attitude with logical reasons, even if these are not appropriate.\n- The speaker questions if a certain behavior is a rationalization to obscure the fact that there is no progress in a particular area. \n- The discussion delves into the impact of competition and the psychological bias towards it.\n\n## Reflection on Competition\n2024-01-18T22:51:02.494Z\n- **Distorted**: Twisted or misrepresented.\n- People are attracted to competition due to finding reassurance in it.\n- Validation is something that needs to be thought through, as the pursuit of something with lots of competition could be problematic.\n- Olympics are seen as better than the pursuit of becoming a movie star as it allows for a quick understanding of one's capabilities.\n\n## Competition and Differentiation\n2024-01-18T22:53:28.494Z\n- **Differentiate**: To recognize or express the difference between things.\n- When objective differences are small, fierce competition arises to maintain a perceived difference.\n- People's identities are often tied to winning competitions, overshadowing what is truly important and valuable.\n- Competition fosters improvement by comparing oneself to others, but may come at a significant cost.\n\n## Reflection on the Importance of Questioning and Exploration\n2024-01-18T22:55:19.497Z\n- Sometimes, focusing on immediate goals can prevent people from exploring bigger questions or alternatives.\n- It's advisable to consider alternative paths that may be less crowded but hold greater potential.\n\n\n## Evolution of Monopoly Businesses and First Movers\n2024-01-18T22:56:06.495Z\n- **Monopoly**: A situation in which a single company or group owns all or nearly all of the market for a given type of product or service.\n- **First mover**: The first company to enter a new market or introduce a new product.\n- Google and Facebook were not the first in their respective fields but excelled due to their innovation and execution.\n- Facebook was the first social networking site to implement real identities, distinguishing it from earlier concepts of cyber identities.\n\n## Rethinking Medicine and Education\n- The question at the end would benefit from more context or clarification. If you provide additional details, I'd be happy to offer possible recommendations.",
      "status": "inactive"
    },
    {
      "title": "359.4 Forecasting",
      "note_id": "11c50331-b42e-46aa-a08d-0665f63b5bf4",
      "markdown": "# 359.4 Forecasting \n\n## Homework Schedule Discussion \n<2024-01-19T02:04:21.528Z>\n- Thursday is designated for homework  \n- Homework due the Sunday of the following week, allowing for a 10-day timeframe  \n- Overlapping with new homework assignments occurs at times   \n\n## Time Series Data and its Importance\n2024-01-19T02:05:53.538Z\n- Time series: Data that is observed or recorded at regular time intervals.\n- Time series data is crucial in various applications, and understanding its analysis is essential for several fields.\n\n## Introduction to Machine Learning Models\n2024-01-19T02:06:53.677Z\n- Neural networks are powerful tools used widely in real-world applications.\n- There are other models that perform well too, such as ARIMA models, regression models, decision trees, random forests, and gradient boosting.\n\n## Forecasting and Prediction\n2024-01-19T02:10:20.745Z\n- Forecasting is used to project the future values of a variable, often applied to time series data such as sales numbers or sensor data.\n- Prediction involves understanding the relationship between variables and making categorical statements about the future, such as predicting customer behavior or identifying patterns in multivariate time series data.\n\n## Multivariate Time Series\n2024-01-19T02:11:53.003Z\n- Multivariate time series involve multiple variables that vary over time, with interactions between them.\n- Autocorrelation: The correlation of a variable with itself at different time lags.\n\n## Macroeconomic effects and commodity prices\n2024-01-19T02:14:14.622Z\n- The **macro effect** refers to a broad, overall trend that affects the economy at a large scale.\n- Real estate prices can be influenced by macro effects, such as a spike in demand due to external factors like economic conditions or global events.\n- **Commodities** are primary goods that are traded in the market, such as gold, silver, and grain.\n- Commodity prices are related to global macroeconomic events, such as natural disasters or geopolitical conflicts.\n- Attempting to predict commodity prices involves analyzing a variety of information, including market trends, news, and historical data.\n\n## Identifying Patterns in Commodity Markets\n2024-01-19T02:16:04.337Z\n- Car dealerships attacked in the Red Sea by rebels causing disruptions in commodity markets.\n- Ships rerouting around Africa affecting commodity prices.\n- Proposal to use machine learning to identify potential patterns and effects on commodities based on news analysis.\n\n## Auto Regressive Integrated Moving Average (ARIMA) Models\n- ARIMA models are used to model future values as a linear combination of past observations.\n- They work well with univariate time series data.\n- The models assume that the time series is stationary, meaning it does not have any global or seasonal trends.\n\nVocabulary:\n- **ARIMA**: Auto Regressive Integrated Moving Average model, used for time series analysis.\n\n## Auto regressive integrated moving average (ARIMA) models\n2024-01-19T02:17:11.928Z\n- ARIMA models are used to model future values as a linear combination of past observations.\n- They are suitable for univariate time series data.\n- They cannot handle seasonal or cyclical components inherent in the data.\n- Stationarity is a key assumption for ARIMA models.\n\n## Auto Regression and Moving Average Models\n2024-01-19T02:21:58.690Z\n- Auto regression (AR) means regressing on the variable itself.\n- Regression on past values is done to predict the current value, i.e., YT from YT-1, YT-2, etc.\n- Moving Average (MA) involves creating a weighted sum of recent values in the time series for the variables based on its past.\n- The model uses a weighted sum, which is a linear combination of past values and recent values of the errors.\n- The errors are represented as white noise, as in YT - YT-1.\n\n## ARIMA Model Components\n2024-01-19T02:21:58.690Z\n- ARIMA model includes **autoregressive terms** and **moving average terms**. \n  - **Autoregressive terms**: These are the past values used in the model.\n  - **Moving average terms**: These are the errors between the time steps, which are essentially white noise.\n\n## Understanding Stationarity\n2024-01-19T02:24:52.549Z\n- Stationary time series: It means that the time series does not have a trend or seasonality.\n- Derivative: The rate of change of a quantity with respect to its independent variable.\n- Discrete data: Data that is separate and distinct, not continuous.\n- ARIMA model: Autoregressive Integrated Moving Average model, a popular statistical method for time series forecasting.\n\n## Google Stock Price Analysis\n2024-01-19T02:26:49.554Z\n- The Google stock price data over 200 days is analyzed for stationarity and trend.\n- Non-stationary time series: Time series with a trend, where the derivative of the time series is non-zero, indicating a consistent pattern of change over time. \n- Stationary time series: Time series where the observations' differences are taken to remove trends. This helps in creating a constant mean and variance over time.\n- Trend line: A line that depicts the general direction and pattern of change in the data.\n\n## Random Walk Model\n2024-01-19T02:28:57.551Z\n- **Oscillating**: Moving back and forth in a regular rhythm.\n- **Consecutive Observation**: A series of observations made in sequential order.\n- **White Noise**: A sequence of uncorrelated random variables.\n- In economic data, differences between times can exhibit characteristics of white noise.\n- The difference between two times in a random walk model is called white noise.\n\n2024-01-19T02:30:01.552Z\n- **Centigradation**: Not a standard term; may refer to centigrade, a scale of temperature.\n- The discussion introduces the concept of white noise in the context of economic data.\n- White noise is characterized by normally distributed error.\n\n## Barriers of Centigration of Water\n2024-01-19T02:30:01.552Z\n- No specific vocabulary or equations mentioned.\n- No other relevant information provided.\n\n## Auto-Regressive Models and Time Series Generation\n2024-01-19T02:31:33.556Z\n- Auto regressive (AR) models are linear regression models for predicting or modeling a time series at the current time step as a constant plus a linear combination of P previous time steps. \n- AR models are a classic and well-functioning model for time series.\n\n## Time Series and ARP Model\n2024-01-19T02:33:08.553Z\n- Time series: A sequence of data points, measured typically at successive points in time.\n- ARP model: Autoregressive process model, a stochastic process used in time series analysis.\n- The time series was created using an AR model and white noise.\n\n## Time Series and Moving Average Model\n2024-01-19T02:34:34.554Z\n- Time series: A series of data points indexed in time order\n- Moving average model: A statistical model used to analyze time series data and smooth out short-term fluctuations to identify trends\n- White noise errors: Random and uncorrelated errors in a time series\n- ARIMA model: Autoregressive Integrated Moving Average model, a popular statistical method for analyzing and forecasting time series data\n\n## Time Series Models and Seasonal Decomposition\n2024-01-19T02:36:07.559Z\n- Time series models use data as parameters for moving average and autoregressive models.\n- Moving average models use five as the parameter.\n- Autoregressive models generate a time series based on white noise, with error terms being white noise at specific time steps.\n- Removing trends and seasonal effects is important for time series analysis.\n\n## Seasonal Decomposition and Sarma Models\n2024-01-19T02:36:59.555Z\n- Seasonal decomposition is a technique to identify frequencies and cyclical functions in data.\n- Similar to Fourier transforms but specifically for identifying seasonal patterns.\n- Sarma models add seasonal terms to the ARMA (AutoRegressive Moving Average) models.\n- Parameters PDQS are used to model seasonal effects, where \"S\" specifies the seasonal period, for example, in monthly data.\n  \n## Seasonal Terms in Aroma Models\n2024-01-19T02:38:42.557Z\n- Aroma models: statistical models used to capture the fragrant characteristics of products, such as food or beverages.\n- Parameters pdqs: specific parameters used in time series analysis, representing the autoregressive order, differences, seasonal autoregressive order, and the seasonal differences.\n- Stationary data: data with constant mean, variance, and autocorrelation over time.\n\n## Data Collection and Preprocessing\n2024-01-19T02:40:16.556Z\n- Temperature is taken daily and there is seasonal data that varies by quarter.\n- The temperature data spans from 1850 to the present, with one value for every month.\n- Anomalies are deviations from the average temperature.\n- The data can be obtained from the National Center for Environmental Information.\n- The temperature data is represented as floats.\n\n## Monthly Temperature Anomalies Data Set\n2024-01-19T02:42:32.555Z\n- The dataset provided contains temperature anomalies for each month since 1850.\n- Anomalies refer to deviations from the average temperature.\n- The data is available on the National Center for Environmental Information government page.\n\n## Identifying Stationary Time Series\n2024-01-19T02:43:55.556Z\n- The key task at hand is to identify if the time series is stationary.\n- **Stationary**: A stationary time series is one whose statistical properties such as mean, variance, and autocorrelation are constant over time.\n\n## Stationary data and Seasonality\n2024-01-19T02:45:54.555Z\n- Data that doesn't exhibit significant changes in statistical properties over time is known as **stationary data**. \n- **Seasonality** refers to a characteristic of time series data where patterns repeat at regular intervals.\n\n## Installing the Stats Model\n2024-01-19T02:47:48.560Z\n- The remark comes with the stats model package.\n- Define the order as 311, where P=1, d=1, and Q=1.\n- This is an autoregressive model with P=3.\n\n## Time Series Analysis and Forecasting\n2024-01-19T02:49:02.559Z\n- Time series data after differencing to achieve stationarity.\n- The red line represents the forecasted values.\n- The model appears to struggle to capture the seasonality.\n\n## Modeling Time Series Data\n2024-01-19T02:51:27.566Z\n- Modeling parameters include a seasonal effect to account for general seasonality over 12 months.\n- The temperature time series data revealed a global trend of increasing temperature and a seasonal effect.\n- The model accurately captured the trend and seasonality, providing a reasonably accurate forecast.\n\n## Components of ARIMA and SARIMA models\n2024-01-19T02:53:51.567Z\n- ARIMA model and its parameters:\n  - **How long do you look back**: Determines the number of past time steps taken into account for forecasting.\n  - **How much difference is required**: Specifies the difference between the time series data to make it stationary.\n  - **How many error terms to take into account**: Specifies the number of steps back in the error process.\n- SARIMA model and its additional parameters:\n  - **Seasonal pattern identification**: Helps identify global seasonality, such as a 12-month season for temperature data.\n  - **Difference parameter for seasonal pattern**: Determines how many times the seasonal difference should be taken into account.\n\n## Identifying Invitations and Using Stationary Models \n2024-01-19T02:54:40.560Z\n- In the code, the first step is to identify whether an invitation is present or not.\n- If the invitation is stationary, a difference is taken. \n- The model used for Army data didn't block at the time series itself, but instead, all differences were fed into the model.\n- The model in use doesn't require taking differences before feeding the data and then fitting the model. \n- The model is then fitted, followed by forecasting. \n\n## Using Neural Networks for Time Series Data Preprocessing\n2024-01-19T02:57:29.563Z\n- Scale the data to fit within the 0 to 1 range to facilitate learning.\n- The data consists of temperature values.\n- To use a neural network on time series data, the series needs to be converted into sequences for supervised learning.\n\n**Vocabulary:**\n- **Supervised learning algorithm**: A type of machine learning algorithm where the model is trained on input-output pairs, and the aim is to learn a mapping from the input to the output.\n\n**Equations:**\n- YT - 1, YT - 2: Observations or input values.\n- Y_target: The value being predicted or forecasted.\n\n## Input and Output Features in Time Series Data\n2024-01-19T02:58:57.567Z\n- Input features and output are present in time series data as well.\n- The value being predicted or forecasted is defined as the target in time series data.\n- Previous values (YT - 1, YT - 2, etc.) are treated as observations or input for forecasting the next time step.\n\n## Recurrent Neural Networks (RNN)\n2024-01-19T03:00:34.570Z\n- RNNs function by taking input data in sequence and can assign a time component to the input data.\n- Classic neural networks lack a sense of time and cannot handle time series data efficiently.\n  \n## Understanding LSTM and RNN\n2024-01-19T03:02:38.555Z\n- LSTM: A type of recurrent neural network (RNN) that is designed to overcome the vanishing and exploding gradient problems. LSTMs are able to learn long-term dependencies.\n- De-emphasize: To reduce the importance or priority of something.\n\n## Differences between Neural Networks and CNNs\n2024-01-19T03:04:05.552Z\n- Time dimensions are a significant difference between neural networks and convolutional neural networks (CNNs).\n- The former (neural networks) do not have any sense of time and are timeless, while the latter (CNNs) are able to process time-related data, which makes them powerful for tasks involving text, speech, and video processing.\n\n## Challenge to Find Pythonic Ways for Performing Operations on Sequences\n2024-01-19T03:06:20.555Z\n- The user wants to find efficient ways to perform operations on sequences using Python.\n- They emphasize the challenge of implementing these operations in an efficient manner.\n- The functionality should allow stepping back by 24 steps in the sequence.\n\n## Using a Normal Neural Network and Feeding Sequences\n2024-01-19T03:07:34.554Z\n- **Normal neural network (NN)**: A type of neural network where the layers are connected sequentially, without any specialized structure.\n- The concept of using a normal neural network in the context of working with sequences is being discussed.\n\n## Reading in Temperature Plot and using Functional API\n2024-01-19T03:10:15.612Z\n- Reading and scaling temperature plot for San Francisco sequencies.\n- Shape of X is one dimensional.\n- Using the functional API to construct the network.\n- Defining input shape on the first layer and assuming the outputs of the first layer are fed into the second layer.\n\nVocabulary:\n- **Sequential API**: A simple way to construct a neural network by adding layers in a sequence, from input to output.\n\nEquations:\n- Loss function: Mean square error (MSE)\n- Optimizer: Adaptive stochastic gradient descent (Adam)\n\n## Functional API Code explanation\n2024-01-19T03:11:20.927Z\n- Code uses the functional API for neural networks\n- Input shape is defined as (you know, comma)\n- Output layer has only one node for forecasting\n- Loss function used is mean square error\n- Optimizer used is Adaptive stochastic gradient descent (Adam)\n- Training on a sample dataset, not the full dataset\n- Number of parameters in the model is 11,500\n\n## Comparison of Neural Networks and Decision Tree Models\n2024-01-19T03:14:38.561Z\n- The speaker wants to compare different models to understand their effectiveness\n- Mentions that neural networks are not a \"Silver Bullet\" and are good for many applications, but not always the most effective\n- Refers to a paper on forecasting competitions at Walmart where neural networks were not the winning models\n- The winning model used a truncated singular value decomposition to remove signal variations of data\n- Highlights that decision tree models are nonparametric supervised learning algorithms, used for both regression and classification tests\n\n### Discussion of Neural Networks and Decision Trees\n2024-01-19T03:16:19.559Z\n- Neural networks don't always outperform other algorithms.\n- Decision tree models are nonparametric but supervised learning.\n  - Nonparametric: Does not assume a set form for the distribution of the data. \n  - Supervised learning: Learning from labeled training data.\n- Decision trees can be used for both regression and classification tasks.\n- Decision trees involve asking questions based on features to make decisions and constructing a tree.\n\n## Decision Trees and Random Forests\n2024-01-19T03:17:48.557Z\n- Decision trees are a non-parametric algorithm used for both classification and regression tasks.\n- Feature vectors: A feature vector is an n-dimensional vector of numerical features that represent an object. In this case, the feature vectors represent the data used to construct the decision tree.\n- Overfitting: Overfitting occurs when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data.\n\n## Decision Trees and Random Forests for Parameter Significance\n2024-01-19T03:18:43.560Z\n- Decision trees and random forests are useful for identifying important parameters.\n- **Ensemble**: A group of items viewed as a whole rather than individually.\n- Ginny index: Might refer to the Gini index, a measure of statistical dispersion.\n- When using decision trees or random forests for time series, the issue of turning the sequence into a time series needs to be addressed.\n\n## Random Forest and Decision Trees\n2024-01-19T03:19:46.560Z\n- Random Forest and Decision Trees are combinations of multiple decisions into one output.\n- Random Forest is an ensemble of decision trees that feed into each other.\n- When using decision trees or random forests for time series, the issue of sequencing needs to be addressed.\n\n## Decision Tree for Time Series Analysis\n2024-01-19T03:22:37.564Z\n- Decision fees: The fees incurred in making a decision. \n- Trends: Patterns or inclinations in data over time. \n- Decision tree: A decision support tool that uses a tree-like model of decisions and their possible consequences. \n- Time series: A sequence of data points indexed in time order.\n\n## Understanding the Process of Scoring and Evaluation\n2024-01-19T03:22:37.564Z\n- Scalar: A term related to scaling, used to normalize the data and bring all features to a similar scale to prevent any feature from being too dominant. The term \"scalar\" refers to a mathematical operation that transforms the data in a consistent way.\n- Inverse trend: Refers to the reversal of a pattern or trend. In the context of data, it signifies the restoration of the original trend after scaling or transformation.\n- Mean square error: A measure of the average squared difference between the actual and predicted values. It indicates the quality of the model's prediction.\n\n## Mean Square Error (09 vs 10 to the -9) \n- The mean square error (MSE) is a measure of the average of the squares of the errors or deviations. \n- Green mean square error is 09 while the decision tree mean square error is 10 to the -9, indicating a significant improvement in accuracy. \n\n## Root Mean Square Error (RMSE)\n2024-01-19T03:23:55.560Z\n- **Root Mean Square Error (RMSE)**: A measure of the differences between values predicted by a model or an estimator and the values observed. It is used for the evaluation of models in statistics and machine learning.\n- Decision tree RMSE: 10^-9, which equates to 0.90\n- The time series is extremely close to the decision tree's performance.\n\n## Gradient Boosting, Random Forests, and Decision Trees\n2024-01-19T03:27:56.568Z\n- Gradient boosting, random forests, and decision trees are related models\n- They are effective for time series problems\n- An article compares gradient boosting and deep learning for credit score prediction in Financial Services\n- The experiment showed that gradient boosting is more powerful than deep learning and has lower computation requirements\n\n## Discussion of Forecasting Techniques\n2024-01-19T03:29:44.563Z\n- **Computation Requirements**: The amount of computational resources needed for a specific task.\n- Retail Forecasting: Predicting future sales and demand for products in a retail setting.\n- Causal Factors: Variables or events that directly influence the outcome of interest.\n- Unitary Sales Forecasting: Predicting sales for individual units of a product.\n- Ensemble: A combination of multiple forecasting techniques or models.\n- Gradient Boosting Trees: A machine learning technique that builds trees sequentially and corrects the errors of the previous trees.\n- Overfitting: When a model learns the training data too well and fails to generalize to new data.\n- Neural Networks: A type of machine learning model inspired by the structure of the human brain.\n- Vanishing and Exploding Gradients: Issues related to the training of neural networks when the gradients become extremely small or large, causing learning to slow down or fail to converge.\n\n## Vanishing Gradients\n- The vanishing gradients problem occurs when the gradients in a neural network become very small as the network approaches the minimum of the loss function.\n- **Vanishing gradients**: This refers to the situation where the derivatives become extremely small, making it challenging for the network to learn effectively.\n- In a computer, mapping a very small number can lead to rounding problems, and the network may not make significant progress in each time step, which is known as vanishing gradients.\n\n## Discussion of Neural Networks and Decision Trees\n2024-01-19T03:32:24.563Z\n- Decision tree: A decision tree is a flowchart-like tree structure where an internal node represents a feature or attribute, the branch represents a decision rule, and each leaf node represents an outcome. \n- Neural networks: A neural network is a series of algorithms that endeavors to recognize underlying relationships in a set of data through a process that mimics the way the human brain operates.\n- Stock price analysis: Comparing the use of decision trees and neural networks in analyzing stock prices.\n- Sequences of data: Refers to the arrangement of data or events in a successive order.\n\n## Google Stock Price During COVID\n2024-01-19T03:33:26.013Z\n- Time series data: Data points collected or recorded at specific intervals of time.\n- Sequences: A set of data points collected or recorded in a particular order.\n\n## Time Series Analysis and Data Splitting\n2024-01-19T03:34:28.472Z\n- Time series analysis involves analyzing data points collected or recorded at regular intervals of time.\n- In time series analysis, the training and test data sets are created according to the sequence of the data.\n- The training data set consists of the first 70% of the time series, and the test data set consists of the last 30% of the time series.\n- Random sampling for the test data set is not suitable for time series analysis, as it may jumble up the sequence of data points.\n\n## Time Series Data Splitting\n2024-01-19T03:35:22.281Z\n- Time series data is split into two parts - train and test data.\n- TensorFlow is used for data splitting and validation.\n- A validation parameter, \"cold equals 3\" is mentioned, indicating 30% of the data to be tested.\n\n## Data Splitting and Preprocessing\n2024-01-19T03:36:37.575Z\n- The data has been split into training and testing sets for model development.\n- The training data set has a length of 51.\n- The testing data set has been created with the same length as the training data set.\n\n## Creating Test Data Sets for RNs\n2024-01-19T03:37:36.576Z\n- Additional dimension needed for RNs\n  - RN: Recurrent Neural Network\n- Need to add the additional dimension into the data set\n- Scaling using the max value\n- Implementation using an RN and a DNN\n\n## Implementing RNN and CNN Models and Training in Keras\n2024-01-19T03:38:58.575Z\n- Implementing RNN (Recurrent Neural Network) and CNN (Convolutional Neural Network) models in Keras for training.\n- Utilizing units return sequence and several layers of RNN in the first model.\n- Mention of theory behind implementing RN (Recurrent Neural Network) and how it is similar to defining DNA units.\n- Intention to build a CNN model with specifications such as \"128 64\".\n\n## Models Compilation and Training Process\n2024-01-19T03:40:32.578Z\n- Models have a compile statement where the optimizer and loss function are defined.\n- Callback hooks are used for metrics tracking.\n- The model is printed and a summary is created.\n- Different models like LSTM and GRU are being experimented with.\n- SGD (Stochastic Gradient Descent) was used instead of Adam optimizer, and it performed better.\n\n## Training and test data scaling\n2024-01-19T03:42:03.581Z\n- Function used to standardize the input features, ensuring all features have the same scale.\n- **Scalar**: A quantity that has magnitude and direction, often used in the context of vector quantities in physics. \n\n## Model Training and Decision Tree\n2024-01-19T03:43:22.577Z\n- Values are printed before running the decision tree.\n- The decision tree works differently and is not defined as part of the model.\n- A function for making predictions and plotting the results is utilized.\n- Decision tree code had to be duplicated as a solution due to difficulty in integrating it with other parts.\n\n## Model Training Results \n2024-01-19T03:44:32.576Z\n- Currently training the LSTM model with 56,000 parameters. \n- The RNN model has already been trained, with the results indicating the Google stock price. \n- The training data consistency has been addressed, resulting in improved performance. \n- The LSTM model has shown improved performance as compared to the RNN model. \n- The decision tree model also demonstrates good performance, though not as fantastic as before. \n\n## Model Comparison and Training Results\n2024-01-19T03:45:24.576Z\n- The models compared are LSTM, decision tree, neural networks, GRU, and R&M.\n- The decision tree performs similarly to LSTM and neural networks.\n- The LSTM model is visualized as the yellow curve.\n- The GRU and R&M models show similar results.\n\n### Time Series Analysis and Machine Learning Models\n2024-01-19T03:46:53.579Z\n- **Models and Loss of Accuracy:**\n  - Four models were trained, and the loss of accuracy for each was observed.\n  - Mention of \"meat update\" - more context needed to interpret this phrase.\n  - The best-performing model was identified as the GRU (Gated Recurrent Unit) model.\n  - Trained models were not large, with approximately 56,000 parameters.\n  - It was mentioned that bigger models could potentially improve performance, but previous attempts with larger architectures did not yield significantly better results.\n  - Time series analysis was described as quite challenging.\n\n## Lecture Overview and Classic Algorithms for Time Series Analysis\n2024-01-19T03:48:40.511Z\n- Classic algorithms for time series analysis included decision trees, random forests, and gradient boosting.\n- Supervised learning algorithms like neural networks require time series data to be converted into sequences for input.\n- Neural networks are structured to expect data in sequences, making them effective for time series analysis and tasks associated with time dimensions such as text understanding.\n- The homework assignment is to train a deep neural network for predicting taxi fares in New York.\n\n\n## Homework Assignment Details\n2024-01-19T03:49:43.238Z\n- The homework involves predicting taxi fares using a deep neural network on the provided dataset.\n- The dataset is available on Blackboard and the task is to predict the fare amount for a ride in New York City.",
      "status": "inactive"
    },
    {
      "title": "Case studies day 5",
      "note_id": "d903fd23-de47-4b32-b1d7-e161c117ae47",
      "markdown": "# Case studies day 5 \n\n## Communication about AI shirts and ski trip\n2024-01-22T17:07:36.072Z\n- Reference to \"AI shirts\" and \"SodaStream\"\n  - **AI**: Artificial Intelligence\n  - **SodaStream**: A machine for making carbonated water at home \n- Mentions of a ski trip and optimization of time\n- Discussion about recording and making inferences\n\n## Conversation about Skiing and Artificial Intelligence\n2024-01-22T17:07:36.072Z\n- Ski shirt: A garment designed for skiing.\n- AI shirts: Clothing related to artificial intelligence.\n- SodaStream: A device used for making carbonated drinks at home.\n- Center for Artificial Intelligence and Society: An organization related to the study and impact of AI on society.\n- Representative sample: A subset of a population that accurately reflects the characteristics of the whole population.\n\n## Hypothesis Testing of Means\n2024-01-22T17:10:43.432Z\n- Sample rate assessment for hypothesis testing \n- Determining the acceptance or rejection of the null hypothesis\n- Null hypothesis: There are no effects\n- Testing for the size of the effects\n\n## Experimental Design and Comparing Results\n2024-01-22T17:11:47.134Z \n- The user is looking for help with how to compare two different groups (test and control groups) to analyze the effects of a medication. They want to understand how to determine if there is a significant difference between the two groups.\n\n## Hypothesis Testing\n2024-01-22T17:13:29.595Z\n- **Hypothesis testing:** A statistical method used to make significant decisions about a population by analyzing sample data.\n- In hypothesis testing, it's crucial to establish a clear null and alternative hypothesis to compare two groups.\n- The significance level, denoted by , is the threshold chosen as the standard for determining whether the results of the hypothesis test are statistically significant.\n\n## Effect and Hypothesis Testing\n2024-01-22T17:16:08.144Z\n- Effect: A change or result that is caused by an event or action.\n- Hypothesis testing: A statistical method used to make inferences about a population parameter based on a sample of data.\n\n## Understanding Normal Distribution\n2024-01-22T17:26:23.603Z\n- Data not following a normal distribution may require a larger sample size.\n- The balance between the minimum number of observations and a larger sample size is crucial.\n- Larger samples can skew results, so it's essential to find the right balance.\n\n## Understanding the Statistical Analysis\n2024-01-22T17:27:48.198Z\n- The speaker is discussing a statistical analysis related to a Central Library and student performance.\n- There is a reference value of 60 and a mention of points available on potassium.\n- The sample size is 15.\n\n## Significance Level\n2024-01-22T17:29:34.986Z\n- Significance level: The level at which the results of a statistical test are considered significant\n- Alpha (): The level of significance, often set at 0.05 in statistical tests\n- If p-value > 0.05, cannot reject the null hypothesis\n- Students' performance above 60 but p-value > 0.05 indicates it's not significant\n\n## Appointment analysis\n2024-01-22T17:31:03.558Z\n- The speaker mentions \"confidence tables\" which might refer to statistical confidence intervals or hypothesis testing.\n- They discuss a value of 60, likely indicating a threshold for significance in their analysis.\n- They are considering whether to reject the null hypothesis based on the test result. \n- They indicate a need for more testing if the value was only slightly above 60, such as 61.\n\n## Hypothesis Testing for the Mean Difference\n2024-01-22T17:32:19.174Z\n- Two-sample hypothesis testing is being discussed.\n- Mention of comparing the means of two classes with the same material but different teaching methodologies.\n\n## Confidence Interval for Difference of Means\n2024-01-22T17:33:32.241Z \n- Confidence interval for the difference of the means of two populations\n  - **Confidence interval:** A range of values within which a population parameter is likely to lie \n  - **Hypothesis:** A statement that is tested for possible rejection under the assumption that it is true\n  - **Random sample:** A sample in which each member of the population has an equal chance of being selected\n  - **Significance level:** A threshold that is used to determine when a null hypothesis can be rejected \n  - **Assumptions:** Conditions that are required to be met for a statistical test to be valid\n  - **Computing means data:** Performing the calculation of the average value of a set of data\n  - **Standard deviation:** A measure of the amount of variation or dispersion of a set of values\n\n## Comparing Distributions and Testing for Significance\n2024-01-22T17:34:50.152Z \n- Comparing distributions is determining if one dataset is different from another. \n- Outliers: Values that are significantly different from the rest of the data.\n- Variance: It measures how far a set of numbers are spread out from their average value. \n- Distribution curve: A graph showing frequency distribution.\n\n## Data Analysis and Interpretation\n2024-01-22T17:36:42.154Z\n- **T value**: The T value is -408, indicating a significant effect.\n- The data suggests that students performed better in a statistically significant way.\n- It implies that there is a need for action or adjustment in the next academic year.\n\n## Understanding Statistics and Educational Measurement\n2024-01-22T17:38:35.156Z\n- **Education**: The process of receiving or giving systematic instruction, especially at a school or university.\n- **Effect**: A change that is a result or consequence of an action or other cause.\n- **Positive 13.74**: Refers to a positive value of 13.74 with no context provided.\n- **Negative numbers**: Refers to numbers less than zero.\n- **Squaring and square root**: Mathematical operations involving raising a number to the power of 2 and then finding the square root, often used to transform data or solve equations.\n- **Population and LA**: The reference to \"population\" and \"LA\" may relate to statistical concepts such as sample population and population mean or to a specific context, but it is not clear from the given text.\n- **Comparing two samples**: The act of assessing the differences and similarities between two sets of data acquired from different sources or under different conditions.\n- The discussion of \"confidence in the event\" implies the consideration of statistical measures such as confidence intervals or p-values.\n- The assertion of \"significant difference\" suggests the evaluation of statistical significance, possibly through hypothesis testing.\n\n## Using Paired T-Test for Student Evaluation\n2024-01-22T17:40:19.159Z\n- Paired T-test is used to compare the means of two related groups to determine statistical significance.\n- It involves testing the same group of individuals at two different times, or under two different conditions.\n- It can be used in the context of student evaluation to compare pre-test and post-test scores.\n\n## Understanding Statistical Analysis\n2024-01-22T17:41:59.155Z\n- Continuous observations are important for comparing data.\n- Sample size requirement needs to be determined.\n- Terms like standard error and standard deviation are relevant.\n- Discussion about the difference in means and the effect of the observations.\n\n## Understanding Significance Testing\n2024-01-22T17:44:12.155Z\n- **Effect**: Refers to the impact or influence of a particular factor or variable on the outcome of an experiment or study.\n- **T value**: The t-value measures the size of the difference relative to the variation in your sample data.\n- Statistically significant: Indicates that the results are unlikely to be due to chance and can be attributed to the variables being studied.\n- Population mean: The average value of a variable in a population.\n- Null hypothesis: A statement that assumes there is no effect or no relationship between variables.\n\n## Statistical Hypothesis Testing\n2024-01-22T17:45:23.157Z\n- **Mean**: The average value of a set of numbers.\n- **One sample t-test**: A statistical test used to determine whether there is a significant difference between the mean of a sample and a known or hypothesized population mean.\n- Mention of \"small p-value\": When the p-value (probability value) is small (usually less than a significance level, ), it indicates strong evidence against the null hypothesis, so the null hypothesis can be rejected.\n\n## Peabody's test results and significant results\n2024-01-22T17:48:36.158Z\n- Peabody's test: A statistical test used to determine whether the means of two groups are significantly different.\n- Not significant results indicate that there is not enough evidence to reject the null hypothesis.\n\n## Hypothesis Testing and T-Distribution\n2024-01-22T18:12:16.172Z\n- Assumption: All hypotheses are correct, T-value is zero, and the probability distribution is centered around zero.\n- Each number is compared to a key value to determine if it is rare or not.\n  \n## Hypothesis testing and two-tailed tests\n- Hypothesis testing involves making a decision based on sample data about whether a specific statement regarding a population parameter is likely to be true. \n- Two-tailed tests consider both positive and negative deviations from the null hypothesis.\n- The p-value is the probability of obtaining results as extreme as the observed results of a statistical hypothesis test, assuming that the null hypothesis is true.\n- The critical value for rejecting the null hypothesis corresponds to the significance level, often denoted as .\n\n## Statistics and Hypothesis Testing\n2024-01-22T18:13:32.171Z\n- Two-tailed tests mean positive and negative include both sides.\n- Percent of the curve that is two or greater or -2 or less needs to be calculated. \n- If the p-value is less than or equal to the significance level (), then the null hypothesis can be rejected.\n\n## Policy Update and Deletion\n2024-01-22T18:47:43.822Z\n- Updating policies based on email for the current element\n- Enabling user update based on email\n- Consideration for potential deletion of elements\n- Discussing the issue of users not being related to a specific user ID\n\n## Access to Databases and Database Security\n- Access control measures for preventing user access to databases\n- Use of triggers or APIs for creating orders and managing database security\n- Implementation of security policies to secure the database\n\n## Access Control and Security Policies\n2024-01-22T18:54:57.453Z\n- Access control: Mechanism that defines who can access what in a system.\n- Security policies: Rules and procedures that define how a system should be protected.\n- Trigger: A set of instructions that automatically execute when a certain condition is met.\n- API: Application Programming Interface, a set of rules that allows different software applications to communicate with each other.\n\n## Difficulty with a Project\n2024-01-22T20:48:11.718Z\n- The speaker is struggling with a project, mentioning \"the amount of black s*** that requires\" and the challenge of measuring everything correctly.\n- They express uncertainty about the weaving process, indicating that cutting and crafting are involved.\n- The speaker expresses frustration about the difficulty of the project and feeling tempted to abandon it.\n\n## Connection Frustration\n2024-01-22T20:49:44.078Z\n- **String**: A sequence of characters.\n- It can be frustrating when trying to connect a string and encountering unexpected difficulties.\n- Consider breaking down the task into smaller steps to make it more manageable.\n\n## Reflection on Language Use\n2024-01-22T20:51:10.442Z\n- **Innovate**: To make changes in something established, especially by introducing new methods or ideas.\n- **Pretentious**: Attempting to impress by affecting greater importance, talent, culture, etc., than is actually possessed.\n- **Rid (of)**: To make a space or place free of something undesirable.\n- **Polluted**: Contaminated, dirty, or unclean.\n- The user expresses a desire to innovate and create change but also feels hesitant about appearing pretentious. They also mention concerns about pollution in the oceans.\n\n## Justin Glass and AI Research\n2024-01-22T20:52:35.420Z\n- Justin is implied to be a person's name mentioned in the conversation.\n- \"Glass\" likely refers to a physical object rather than a person named \"Justin Glass\".\n- The mention of \"AI research\" indicates a focus on artificial intelligence development or study.\n\n## Troubleshooting the Mic Issue\n2024-01-22T20:53:41.620Z\n- Secondary fail safe: A backup mechanism to activate the system in case of the main method failing.\n- Websockets connection: A communication protocol that allows real-time data transfer between a client and a server.\n- Utilizing the websockets connection for the mic.\n\n## Discussion about Shark Population\n2024-01-22T20:56:24.069Z\n- Increased shark population due to feeding off Fisherman's catches, leading to overeating and reproducing.\n- This phenomenon is becoming a significant problem and affecting the ecosystem.\n\n## Shark Population Growth\n2024-01-22T20:56:24.069Z\n- Shark population has grown significantly in recent years due to increased food supply from Fisherman's catches, leading to more eating and reproducing.\n- This growth has become a major problem and is impacting the fish population and fishermen's catches.\n- The imbalance in the ecosystem caused by the increased shark population is raising concerns among conservationists and fishing enthusiasts.\n\n## Conservation and Fishing Enthusiasts\n2024-01-22T20:57:20.075Z\n- Conservationists: individuals dedicated to the protection of the environment and wildlife.\n- Fishing enthusiasts: people who have a strong interest in the activity of fishing.\n- Discussion about the impact of fishing on fish and shark populations, and the need for regulations and funding in conservation efforts.\n  \n## Null Hypothesis and P-Value\n2024-01-22T20:59:35.070Z\n- **Null Hypothesis**: The initial assumption that there is no significant difference or effect.\n- **P-value**: A measure of the strength of the evidence against the null hypothesis. It measures the probability of obtaining test results at least as extreme as the ones observed during the test, assuming that the null hypothesis is true.\n- The range of the P-value corresponds to the range being swept of the bell curve.\n- The null hypothesis is likely to be located within this range.\n\n## Discussing Math and T-tests\n2024-01-22T21:01:48.068Z\n- The speaker is trying to understand math and mentions confidence intervals and p-values.\n- They are also discussing t-tests and seem to be trying to break some code while learning about them. \n\n## Conversation about App Development\n2024-01-22T21:03:22.068Z\n- \"Build opal\" likely refers to the development of an app called Opal\n- MVP stands for \"Minimum Viable Product\", which is the first version of a product with just enough features to satisfy early customers and provide feedback for future development\n- Deadline for sending Opal to the Apple Store: Next Monday 29th\n\n## Discussion with SCP Member\n2024-01-22T21:05:17.071Z\n- The individual mentioned an \"alumni\" - **alumni** refers to graduates or former members of an institution.\n- They discussed a \"retreat\" that \"Cole\" was on - **retreat** refers to a period of seclusion for the purposes of prayer and meditation or a place to which one may retire. \n- Mention of \"former alumni\" having an impact on the project.\n- Dialogue about the projects each person was working on.\n\n## Clarification on Vocabulary\n- **SCP:** Stands for \"Student Consulting Projects,\" a program involving student consulting projects to contribute to a team. \n\n## Understanding the Recruitment Process\n2024-01-22T21:07:44.309Z\n- SCP: Stands for **\"Selective Campus Program\"**, a recruitment process.\n- Pickleball networks: This may refer to a network of people interested in playing pickleball, a racquet sport. \n- Coffee chats: Refers to a scheduled meeting or discussion over coffee, potentially related to networking or recruitment.\n- Business deliverables: Items expected to be created and delivered as part of the recruitment process.\n\n## Coffee Shop Discussions\n2024-01-22T21:11:04.784Z\n- Coffee shops are not mandatory, but they are a mandatory social event.\n- Participants will meet all of the actors and have their \"Vibes\" assessed.\n- Rejected talented individuals in the past due to potential community fit.\n- Only those who have interacted with participants in a professional sense can vote on Friday.\n\n## Exploring Networking Opportunities\n2024-01-22T21:12:19.682Z\n- *Networking*: The process of interacting with others to exchange information and develop professional or social contacts.\n- The speaker expresses a desire to meet more people and gain insight into the application process.\n- The idea of spending time sorting through a conflict of interest document is mentioned, indicating a potential professional setting.\n\n## Novo the Coffee House\n2024-01-22T21:13:49.069Z\n- **People Skills**: The ability to communicate and interact effectively with others.\n- **Motivates**: The reasons or goals behind someone's actions.\n- Elephants walking is not a commonly known term, perhaps it's an industry-specific expression. \n- Mention of \"recruitment team\" implies a possible organizational context.\n\n\n## Work Buddies and Pictures\n2024-01-22T21:17:20.684Z\n- The phrase \"work buddies\" suggests a friendly or collaborative work environment.\n- The mention of \"adjusting the font size\" may be related to document formatting or design.\n- Reference to \"The Magic Touch\" may indicate a sense of accomplishment or satisfaction.\n- The context of \"falling\" and \"work for 20 minutes\" may refer to physical activities or work tasks.\n- There is no specific vocabulary or technical information mentioned in this message.\n\nEquations: None mentioned.",
      "status": "inactive"
    },
    {
      "title": "Mark Z facebook",
      "note_id": "f7eb56e1-8168-42a1-8d36-61aef0cec1ce",
      "markdown": "# Mark Z facebook \n\n## Introduction to Computer Science in the Real World\n2024-01-22T22:44:51.955Z\n\n- **Computer Science**: The study of the principles and use of computers.\n- Mark Zuckerberg, founder of Facebook, to discuss computer science and its real-world applications.\n- Facebook's influence in over 2,000 schools globally.\n- Zuckerberg to share background and importance of computer science.\n\n## Mark Zuckerberg Guest Lecture - Background and Importance of Computer Science\n2024-01-22T22:44:51.955Z\n- Mark Zuckerberg is the founder of Facebook, a social networking platform used by over 2,000 schools across the nation.\n- He will discuss the importance of computer science and the real-world applications of this field.\n\n## Lecture at Harvard and Courses Taken\n2024-01-22T22:46:05.958Z \n- Lecture at Harvard discussing courses and impact of decisions made while at Facebook.\n- Courses taken at Harvard include CS and engineering.\n- Started off with course 121 and never took course 50.\n- Roommate Dustin Moskovitz was involved in helping with the expansion.\n- Launched the site at Harvard in February 2004.\n\n## Facebook's Early Days and Technical Challenges\n2024-01-22T22:47:29.964Z\n- Started off writing the site in PHP, similar syntax to C.\n- Launched at Harvard in February 2004.\n- Received requests from other colleges to launch at their schools.\n- Faced challenges balancing coursework (e.g., CS121, CS161) and developing Facebook.\n\n## Decision to use PHP over Pearl\n2024-01-22T22:48:52.972Z\n- PHP: A popular server-side scripting language primarily used for web development.\n- Pearl: A high-level, general-purpose programming language.\n  \n## Exponentially Difficult Network Connections\n2024-01-22T22:49:35.974Z\n- The process of finding friends of friends or the closest connection becomes increasingly difficult at each level, with an exponential increase due to the number of friends each person has.\n- The complexity grows due to the expanding number of friends at each level, with a factor of n, where n is the number of friends a person has.\n- Searching for a friend of a friend is in O(n^2) complexity, and looking for a friend of a friend of a friend increases the complexity further.\n\n## Database Distribution Decision \n2024-01-22T22:51:19.517Z\n- Decided to distribute the databases and create one instance of mySQL database for each school on the network\n- This decision was made to make computation more manageable, with computations only within each school rather than the entire user base\n- Reduced the computation from n cubed over 6 million to n cubed over 10,000, making it a much more feasible computation\n- This architectural decision was crucial in preventing system failure \n\n## Apache and MySQL Distribution and Performance\n2024-01-22T22:52:57.978Z\n- Running the database and web server on Apache for serving pages from the same machine.\n- Distributed databases allowed for linear addition of machines without exponential expansion.\n- Discovered the need for better MySQL and Apache performance after reaching around 30-50 schools.\n- Running MySQL and Apache on the same server caused issues when the server failed, leading to unresponsive databases and web pages.\n\n## Infrastructure Architecture\n2024-01-22T22:53:51.972Z\n- The separation of web servers from database servers to allow uniform access to web servers while maintaining consistent database performance.\n- Using a pool of Apache web servers for load balancing.\n- Dealing with performance bottlenecks when reaching high traffic volumes.\n\n## Traffic Performance and Database Bottlenecks\n2024-01-22T22:56:24.977Z\n- Open source applications used for traffic management and database handling\n- MySQL: open source database, performant but started facing bottlenecks at around 100 million pages a day\n- Bottlenecks: typical query on MySQL taking 2-4 milliseconds, increased to 30-50 queries per page view, leading to the development of a caching layer\n- Caching: initially used memcache for quicker access (0.3-0.5 milliseconds), but faced distribution issues and lacked redundancy in case of box failure\n\n## Reflection on Technology and Innovation\n2024-01-22T22:59:08.974Z\n- Mentioned the outgrowing of MySQL and the implementation of extra redundancy.\n- Emphasized the leverage individuals have in today's technological landscape.\n- Appreciated the evolution of technology, highlighting the transformation from the need for expensive hardware to the ability to rent machines at a fraction of the cost.\n\n## Rise of Leveraging and Technology in Business\n<2024-01-22T23:00:49.976Z>\n- **Leverage**: Using a small amount of resources to control a larger amount of resources or assets.\n- Traditional business problems like raising capital have been mitigated due to the ability to leverage technology and resources more effectively.\n- Businesses are able to achieve more with a smaller initial investment, thanks to technological advancements.\n- Companies like Google and Facebook have achieved massive scale with relatively smaller teams and resources, which was not possible in the past. \n\n## Leveraging Technology for Business\n2024-01-22T23:00:49.976Z\n- Leverage: **To use something to maximum advantage**\n- Technology has enabled businesses to do more with less capital\n- Google and Facebook are able to reach a vast number of people with relatively few employees and machines\n- Advancements in technology have led to completely different business architectures in a short span of time\n\n## Discussion on making technology open source\n2024-01-22T23:02:02.978Z\n- Making technology open source requires significant effort\n- Consideration of losing competitive advantage\n- Challenges related to support and licensing\n- Open source search server considered for release but abandoned due to complexities\n\n## MySQL search optimization\n2024-01-22T23:03:38.979Z\n- MySQL: MySQL is an open-source relational database management system.\n- Lagging: Falling behind, unable to keep up.\n- Distributed machines: A network of independent machines working together towards a common task.\n\n## Importance of Hiring Intelligent People \n2024-01-22T23:04:16.980Z\n- Hiring intelligent people can be crucial in maximizing productivity and problem-solving ability within a company.\n- Small companies can benefit from their agility and reduced bureaucracy to achieve goals efficiently.\n- Smart individuals can leverage the lack of corporate red tape to innovate and drive impactful developments.\n\n## Characteristics of successful companies\n2024-01-22T23:06:51.982Z\n- Companies that end up being successful often start with the goal of making something cool rather than starting out with the intention of building a successful company.\n- Examples of successful companies that started this way include Google, Yahoo, eBay, and Amazon.\n- These companies had humble beginnings and did not necessarily seek a lot of advice early on.\n\n## The Importance of Core Ideas in Company Development\n2024-01-22T23:08:52.979Z\n- Companies often start with a core idea that drives their initial success.\n- Yahoo's original concept was organizing information by directory.\n- Google's core idea was page rank, from which their search function developed.\n- Facebook's unique feature was the ability to search for information about individuals.\n\n## Starting a Core Idea and Surrounding Yourself with Smart People\n2024-01-22T23:09:51.983Z\n- Core idea: the central concept or main principle of a project or business\n- Surrounding yourself with smart people: having intelligent and capable individuals as part of one's professional network\n- The success of a company can depend on the unique understanding of the core idea by its founders and early employees\n- Startups benefit from having a team with diverse expertise to complement the founders' understanding of the core idea\n\n## Linking Profiles and Usage Context\n2024-01-22T23:11:49.987Z\n- **Ubiquity**: The state of being widespread or constantly encountered.\n- The idea of linking profiles in a way that requires a ubiquity of usage to understand the context.\n- The process for filtering what technologies to use is based on trusting smart people in the company.\n  \n## Evaluating storage options\n2024-01-22T23:13:14.990Z\n- The team considered supporting unlimited photo uploads.\n- The concept of \"unlimited\" storage was discussed.\n- Evaluation of companies providing large storage solutions like NetApp and Network Appliance was initiated.\n\n## Network Issues and Solution with Java Applet and ActiveX Control\n2024-01-22T23:14:57.989Z\n- Network issues due to router limitations and slow upload speeds\n- Developed a Java applet and an ActiveX control for client-side photo compression\n- Allowed for faster photo uploads and reduced CPU usage on server side\n\n## Equipment Offloading and File Transfer\n2024-01-22T23:15:36.991Z\n- Offloading equipment refers to transferring the responsibility or burden of certain tasks or items to others.\n- Transferring large files involves moving data of significant size from one location to another.\n- Anticipating issues and making decisions efficiently are crucial in executing such tasks.\n\n## Cambridge Analytica Awareness in Facebook Leadership\n2024-01-22T23:17:24.985Z\n- Cambridge Analytica: A now-defunct British political consulting firm that was involved in a scandal where it harvested personal data from millions of Facebook profiles without their consent.\n- March 2018: The time when the Cambridge Analytica scandal became public.\n- Cheryl Sandberg: The Chief Operating Officer (COO) of Facebook.\n\n\n## Understanding Content Policy on Facebook\n2024-01-22T23:18:44.986Z\n- Facebook's policy on fact-checking content\n  - Fact-checking: verifying the accuracy of information presented in the content by cross-referencing it with credible sources.\n- Usage of Census Data for targeting\n- Newsworthiness of content and its impact on policy application\n- Handling of political advertisements on Facebook\n- The future direction and possibilities of technology on Facebook\n\n2024-01-22T23:20:28.993Z\n- Running advertisements targeting specific political affiliations on Facebook\n- The scope and implications of not fact-checking political advertisements\n- The potential of technology to enable multiple individuals to interact virtually in a room",
      "status": "inactive"
    },
    {
      "title": "Z fellows notes with Baylor",
      "note_id": "b34f98b8-45b0-4d57-9c3a-347ab192a5a2",
      "markdown": "# Z fellows notes with Baylor \n\n## Personal Update\n<2024-01-23T17:33:38.379Z> \n- Participant is currently at the Academy.\n- Participant is taking a class on applied neural networks.\n- Participant also attends the University of Southern California (USC).\n- Participant is a freshman.\n- Mentioned meeting someone named Laura.\n\n## USC AI Curriculum\n2024-01-23T17:33:38.379Z\n- ITP359: Applied Neural Networks\n  - The course focuses on applied neural networks and AI, which covers advanced topics in machine learning algorithms and neural network architectures.\n- The student mentioned that they are pursuing a comprehensive AI curriculum at the University of Southern California.\n\n## Meeting Wells\n2024-01-23T17:35:39.378Z\n- Academy faculty member - possibly an associate Dean\n- Offered a room to someone named Wells\n\n## Personal Challenges and Transition to Technology\n2024-01-23T17:36:52.377Z\n- **Dyslexia**: A learning disorder that affects reading, writing, and spelling.\n- **Fine motor skills**: The ability to make movements using small muscles, usually in fingers and hands.\n- Typing efficiency and coding were challenging due to dyslexia.\n- Transitioned from debate to pitching during high school.\n- Developed an interest in AI and technology after high school.\n- Enjoyed creating physical products and selling laser cut goods in high school.\n\n## Personalized Laser Cut Goods Business\n2024-01-23T17:37:40.378Z\n- Sold personalized laser-cut goods using a Universal Laser Systems machine\n- Enjoyed the challenge of transitioning from two dimensions to three dimensions in the design process\n- Developed a website using HTML and CSS to showcase products\n- Donated some of the products, amounting to thousands of dollars in value\n- Found limitations in scalability due to potential defects in wood and the time-consuming nature of the laser cutting process\n\n## Journey into Software Development\n2024-01-23T17:42:15.608Z\n- Deaf class: HTML and CSS\n- Made products\n- Donated products\n- Learned Python after school\n- Developed software for T-Mobile internship\n- Interest in voice interactivity and note-taking\n- Developed live note-taker for real-time transcription\n\n\n## Learning Styles and Note-taking Challenges\n2024-01-23T17:46:58.568Z\n- Struggle with absorbing information by listening or reading\n- Difficulty in focusing on teacher's words and note-taking simultaneously\n- Plans to allow users to add their own notes to the live transcription\n- Meeting with the office of accessibility services to discuss the tool's potential \n- Comparison of existing audio transcription services ($16 per minute) vs. proposed tool (break-even at 5 cents per hour)\n- Planned focus on accessibility to establish a strong foothold in the market\n- Competitive analysis of transcription services, emphasizing real-time and personalization benefits",
      "status": "inactive"
    }
  ]
}
