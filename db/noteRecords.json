{
  "noteRecords": [
    {
      "title": "intro to python",
      "note_id": "c5beeea6-4b22-40c5-a6bb-5b2f19132a59",
      "markdown": "# intro to python\n\n## Overloaded Operators\n<2024-01-15T04:41:49.172Z> \n- Operators can be overloaded, allowing them to behave differently based on their context.\n- Example: combining a number with a string using multiplication will result in a new string with the specified number of replicas.\n- Example: two strings can be combined using the addition operator.\n\n## Static Semantics\n<2024-01-15T05:11:23.224Z>\nIn the context of programming, static semantics is highlighted as an important factor. It involves the analysis of a program's text to determine its meaning, even before it is run.\n\n\n## Type Checking in Python\n<2024-01-15T05:12:24.228Z>\n- Python performs type checking to detect data type-related errors.\n- Inconsistencies in type checking can lead to issues downstream.\n- Different programming languages exhibit a spectrum of type checking, ranging from weak to strong typing.\n\n<2024-01-15T05:13:16.223Z>\nThe lecturer highlights the differences in type checking in Python compared to other languages, especially those with weak typing.\n\n<2024-01-15T05:14:19.224Z>\nThe lecturer delves into an example to demonstrate Python's type checking behavior using the \"less than\" operator, where unexpected comparison outcomes occur between strings and numbers.\n\n<2024-01-15T05:15:19.227Z>\nThe lecturer emphasizes the importance of exercising type discipline in coding to ensure the correct usage of operators and operands.\n\n<2024-01-15T05:17:02.223Z>\nFurther discussion involves the unconventional meanings of certain operators in Python, such as division and the order of operations for arithmetic expressions.\n\n<2024-01-15T05:18:10.218Z>\nThe lecturer continues the discussion about the order of operations, emphasizing the need to be explicit when specifying the desired order of evaluation in expressions.",
      "status": "inactive"
    },
    {
      "title": "MIT lecture 3",
      "note_id": "7dfafac7-faec-4c06-987f-40e4f4e1cee6",
      "markdown": "# MIT lecture 3\n\n## Programming Basics\n<2024-01-15T05:19:32.217Z> \nThe lecturer begins by summarizing the content covered in the previous lectures. They outline the basic elements of programming, including data (numbers, strings, and booleans), operations (such as addition and multiplication applicable to numbers and strings), and commands/statements for changing the flow of control. They also introduced loop mechanisms and emphasized the power of these fundamental instructions for writing common patterns of code.\n\n## Importance of Good Programming Style\n<2024-01-15T05:23:13.783Z>\nThe lecturer highlighted the significance of good programming style, emphasizing the following key points:\n- Use of comments for code clarity and debugging.\n- Type discipline to ensure expected types for operations.\n- Descriptive and meaningful variable names.\n- Testing all possible branches within the code.\nThe focus was on promoting good programming practices to write high-quality code.\n\n\n## Iterative Programs\n<2024-01-15T05:24:32.240Z>\nThe lecture concluded with a hint at building upon the basics discussed previously. The lecturer indicated the intention to delve into iterative programs, focusing on patterns of code that tackle certain classes of problems and tools to aid in understanding those pieces of code. Emphasis was placed on the selection of variables for counting within a process.\n\n<2024-01-15T05:25:24.748Z>\nThe lecturer outlined the key steps for creating an iterative program: Initializing the variable, setting up the right end test to determine loop completion, ensuring the block of code inside the loop contains instructions that change the variable being counted, and deciding what to do when the loop is done.\n\n<2024-01-15T05:26:25.548Z>\nA reminder was given that the block of code in an iterative program represents the set of instructions to be executed each time through the loop and highlighted the necessity of changing the counting variable within the block of code to avoid infinite loops. The importance of having a clear structure when mapping a problem into an iterative program was emphasized.\n\nThis section gave a high-level overview of the considerations and steps involved in creating iterative programs, with a focus on the manipulation of variables within the iterative process.",
      "status": "inactive"
    },
    {
      "title": "ML3",
      "note_id": "f9c649d2-8423-4368-a41a-e7ee7f24e65b",
      "markdown": "# ML3 \n\n## Machine Learning Review\n2024-01-17T01:46:35.872Z\n- Supervised learning involves feature extraction and learning algorithms\n- Feature extraction converts examples into a feature vector\n- Linear predictors or neural networks provide scores for the feature vectors\n- Scores are defined via simple or more complex linear combinations\n- Score functions can be used for classification and regression\n\n## Back Propagation Example and K Nearest Neighbors\n2024-01-17T01:46:35.872Z\n- Back propagation exemplifies how neural networks update weights during training.\n- K Nearest Neighbors is an algorithm used for classification and regression based on the proximity of data points.\n\n## True Objective of Machine Learning\n2024-01-17T01:47:35.473Z\n- Minimizing error on unseen future examples\n- Learning about machines\n- **Machine Learning Vocabulary**\n    - *Gradient*: A vector that points in the direction of the greatest rate of increase of a function and whose magnitude is the slope of the function in that direction.\n- The true objective is minimizing error on unseen future examples, rather than just minimizing error on the training set.\n- Unsupervised learning will be discussed later.\n\n\n## Training Loss as an Objective Function\n2024-01-17T01:51:21.884Z\n- Switching gears to talk about unsupervised learning.\n- Questioning whether training loss is a good objective function.\n- The described algorithm suggests outputting the weight seen in the training set, indicating a need for objective functions beyond simply minimizing training loss.",
      "status": "inactive"
    },
    {
      "title": "ITP 359 lecture 3 Review of supervised ML",
      "note_id": "1773f188-fafe-463e-861d-56fea9219a4f",
      "markdown": "# ITP 359 lecture 3 Review of supervised ML\n\n## Troubleshooting Testing (2024-01-17T02:00:52.619Z)\n- Testing was done before arrival but encountered an error.\n- Describing need for uninterrupted functioning and optimization.\n- Looking to address refreshing issue when using effects and location hook.\n- Working on proper re-rendering of the system.\n- Referring to seller's rooms in the discussion.\n\n## Testing and Error Handling\n- Testing the functionality before the official use to identify and resolve any potential errors.\n- In this context, \"error\" refers to an unexpected problem or malfunction in the system.\n\n## Testing and Debugging\n2024-01-17T02:00:52.619Z\n- Testing a feature before it's ready\n- Encountering errors while testing\n- Needing to optimize the feature\n- Difficulty with refreshing and re-rendering when using location hook to pull data from a link\n\n## Review of Last Semester - 2024-01-17T02:02:50.993Z\n- Next week's session will focus on forecasting.\n- The upcoming topics include Markov chains and naive Bayes outboard.\n- These concepts are important statistical methods.\n\n## Sequential Data in Finance and Audio Processing\n2024-01-17T02:04:42.744Z\n- Algorithmic traders often use sequential data for trading, which is also utilized in audio processing.\n- Hidden Markov models, autoencoders, and variational autoencoders are popular in processing sequential data for various applications.\n\n## AI Approaches and Modeling Techniques\n2024-01-17T02:06:10.989Z\n- Generated AI mentions the use of generators in AI models\n- Plans to start with variation oil and odors in those types of models\n- Homework on forecasting to be completed within two weeks\n- The first homework may be released on Thursday or Monday/Tuesday\n- Emphasis on not rushing through the material and having ample time for learning\n\n## Approximation Problem and AI Approaches\n2024-01-17T02:07:39.014Z\n- **Approximation Problem:** A problem involving finding a function that approximates a given target, often tackled using statistical or numerical methods.\n- AI approaches primarily involve dealing with approximation problems.\n- Techniques for addressing approximation problems include statistical methods, numerical methods, linear regression, logistic regression, and neural networks.\n\n## Regression and Classification\n2024-01-17T02:09:09.171Z\n- Neural network used for classification and regression\n- K-Nearest Neighbor algorithm used for classification and recommendation systems\n- Brief intro to Association Analysis and Principal Component Analysis (PCAs)\n\n## Traveling Salesman Problems and Research Competition\n2024-01-17T02:10:14.064Z\n- Traveling Salesman Problems involve finding the shortest possible route that visits each city exactly once and returns to the original city\n- Research competition likely involved exploring algorithms and strategies for solving complex problems like the Traveling Salesman Problem\n- Statistical standpoint refers to analyzing data from a mathematical perspective, considering patterns and probability\n- Machine learning standpoint involves using algorithms and statistical models for computers",
      "status": "inactive"
    },
    {
      "title": "ITP 359 lecture 4 Neural Networks",
      "note_id": "856f1b5f-39b5-48c9-a549-b526b76ef6bc",
      "markdown": "# ITP 359 lecture 4 Neural Networks \n\n## Neural Networks and Approximators <2024-01-17T03:15:13.293Z>\n- Neural networks as approximators - The neural network is used as an approximator by approximating a function with the help of learning from training examples.\n- Basic idea of neural networks - Each node in the network uses a linear combination of feature vectors and associated weights.\n- Vocabulary: \n  - **Approximator**: In machine learning and statistics, an approximator is a method or learning algorithm that seeks to approximate a target function through training on example inputs, and adjusting the model's parameters to minimize the difference between the predicted and actual outputs.\n\n## Understanding the Perceptron Training Process\n2024-01-17T03:20:32.067Z\n- Perceptron training minimizes misclassifications\n- Misclassifications are minimized using absolute value\n- Weight updating is based on data points and their associated calculations\n- Learning rate multiplied by the distance between the approximation and the actual value\n\n## Training a model\n2024-01-17T03:21:48.840Z\n- Start with the weights being small, not too big to avoid issues with training.\n- Perform steps for each data point, including calculating the linear combination, applying an activation function, and updating the weights.\n\n## Perceptron Training Process \n2024-01-17T03:22:59.749Z\n- Updating weights in each step for each data point\n- Calculating the error and updating weights until a very small error is achieved\n\n## Linearly Classifiable Data and Neural Networks\n2024-01-17T03:25:44.088Z\n- Linearly classifiable data: Data that can be separated into different classes using a straight line or hyperplane\n- Neural networks: A series of algorithms that endeavors to recognize underlying relationships in a set of data through a process that mimics the way the human brain operates\n\n## New AI Architecture\n2024-01-17T03:27:15.177Z\n- The new AI architecture on Netflix involves a dual network with multiple layers. \n- It utilizes activation functions such as sigmoid function, real or logistics function, and hyperbolic tan function to replace the step function.\n- The architecture emphasizes the use of non-linearities in constructing multilayer neural networks.\n\n## Neural Network Definition\n2024-01-17T03:29:09.085Z\n- The network is defined as starting at the end and working backward, with the output being an activation function of the output weights times the output of the final layer. \n- Each layer is defined by a number of \"Lambda\" nodes, with all nodes connected to each other, creating a fully connected feedforward network.\n\n## Neural Network Structure\n2024-01-17T03:32:14.821Z\n- The output of a neural network depends non-linearly on the last layer, and this layer depends on the previous layer and so on, creating a recursive structure within the network.\n- The network can be visualized as an onion, starting with the output and then working back to the input, with calculations being made as it progresses backward through the layers.\n\n## Universal Approximation Theorem\n2024-01-17T03:33:17.509Z\n- The universal approximation theorem, first articulated in 1989, states that a feedforward neural network with a single hidden layer containing a finite number of neurons can approximate any continuous function on compact subsets.\n- Compact subsets are subsets that include their boundary and do not have to satisfy specific constraints on their nature.\n\n## Multi-class Classification and Softmax\n2024-01-17T03:35:46.043Z\n- The raw output of the network is passed through a sigmoid function, mapping the values to a range between 0 and 1, to obtain the probability for each class in a multi-class classification.\n- The issue with using the sigmoid function alone is that the probabilities assigned to each class do not necessarily add up to one, hence creating ambiguity in classification.\n- This method is referred to as multi-class classification, where multiple classes can be associated with the input.\n\n## Sigmoid and Softmax Functions\n2024-01-17T03:37:29.206Z\n- Sigmoid function used in final layer for multi-class classification\n    - Sigmoid function does not ensure probabilities add up to one\n    - Allows for classification into multiple classes\n- Softmax function exaggerates differences in probabilities and ensures sum of probabilities equals one\n    - Enables exclusive belonging or mapping to a class\n\n## Softmax and Multi-Class Classification\n2024-01-17T03:38:30.789Z\n- Softmax ensures that the output values add up to one, which is essential in binary classification to obtain a classification probability.\n- In multi-class classification, you may want to compare probabilities between different outcomes, such as distinguishing between various objects in an image.\n- Softmax exaggerates the distances between classes, making them not directly comparable, which might not be suitable if you want to rank the different classes based on their probabilities.\n\n## Softmax and Cross Entropy Loss\n2024-01-17T03:40:46.085Z\n- Softmax: A function that takes as input a vector of K real numbers, and normalizes it into a probability distribution consisting of K probabilities.\n- Softmax is used in neural networks to provide probabilities for the different classes in a classification problem.\n- Cross Entropy Loss: A loss function used in neural networks when the output is a probability value. It measures the performance of a classification model whose output is a probability value between 0 and 1. \n\n## Cross Entropy Loss\n2024-01-17T03:42:03.824Z\n- Cross entropy loss is very similar to the negative log likelihood and is used in models.\n- In the case of two classes, cross entropy loss becomes the same as the negative log likelihood equation.\n- The derivation of cross entropy loss comes from taking the logarithm and maximizing the likelihood function.\n\n\n## Softmax Activation Function\n2024-01-17T03:43:11.596Z\n- The softmax activation function is used as the final activation function, providing output probabilities that add up to one.\n- It helps in determining a clear winner among the classes in the model.",
      "status": "inactive"
    },
    {
      "title": "Search 1",
      "note_id": "913e5434-806a-473d-aed2-3d8b5397defb",
      "markdown": "",
      "status": "inactive"
    },
    {
      "title": "Descriptive Data Mining",
      "note_id": "d2ff3d8b-92dc-45ca-8152-81736e53a2f6",
      "markdown": "# Descriptive Data Mining \n\n## Data Mining\n<2024-01-17T17:08:45.764Z> \n- Data mining is the process of analyzing large sets of data to discover patterns, trends, and relationships.\n- With the advancement of technology, we now have the capability to extract valuable insights from the vast amount of data that is being generated.\n\n## Data Mining\n2024-01-17T17:11:47.830Z\n- Data mining is the process of analyzing large sets of data to discover patterns, trends, and relationships.\n- It involves extracting useful information from a large amount of data, often using automated methods or algorithms.\n- Companies use data mining to gain insights into customer behavior, market trends, and other business-related information.\n\n## Interest Rate Period\n2024-01-17T17:11:47.830Z\n\n- Interest rate period is the duration for which a specific interest rate is applicable for a financial product.\n- In AI, data is highly valuable for companies to derive insights and make informed decisions. \n- The process of mining and analyzing data allows companies to extract actionable information and drive value. \n\n## Algorithm and Qualitative Assessments\n2024-01-17T17:13:02.993Z\n- Algorithm: A set of rules or processes to be followed in calculations or problem-solving operations, especially by a computer.\n- Qualitative assessments: Evaluations that are based on qualities or characteristics rather than on quantity or numerical data.\n- Consider the use of qualitative assessments in decision-making processes.\n- Discuss the potential impact of qualitative assessments on business decisions.\n\n## Cluster Analysis and Similarity\n2024-01-17T17:14:11.304Z\n- **Cluster Analysis**: A method of grouping objects based on the similarity of their attributes or characteristics.\n- Data preparation is underway.\n- Two ways of dealing with daily money have been mentioned.\n- Mention of clustering of similarity for computer reports and association rules.\n\n## Computer Grouping and Data Analysis\n- The process of grouping data for significance and analysis.\n- Utilizing computer algorithms to group similar data together for analysis and decision-making.\n- Methods for determining similarity and measuring distance between data points.\n\n## Grouping data and measuring similarity \n- Date-time: 2024-01-17T17:15:36.708Z\n- The data is grouped to find significance and aid in decision-making processes.\n- The computer can identify similarities between data using numerical comparisons.\n- Similarity can be measured by comparing numbers, such as age and income, using methods like z-scores and standard deviation.\n\n## Scale and Range of Values\n2024-01-17T17:17:59.707Z\n- Range of values: The difference between the minimum and maximum values within a dataset.\n- Age versus income scale: Income is in the tens of millions while age has a lesser range.\n- Z-scores: Standardized scores that measure the distance of a data point from the mean in terms of standard deviations.\n- Using z-scores to fix scale issue and bring values to the same scale for comparison.\n\n## Z-Scores Explained\n2024-01-17T17:19:44.396Z\n- Z-Score: A measurement of a value's relationship to the mean of a group of values, measured in terms of standard deviations. It indicates how many standard deviations a data point is from the mean.\n- Discussion of using Z-scores to analyze income data and how it can affect the distance equation in analysis.\n\n## Analysis of Distance and Z-scores\n- 2024-01-17T17:20:55.708Z\n- The computation of distance between data points is essential in analysis\n- Z scores dominate the distance equation, which can skew the analysis\n- Z scores transform data values to represent their distance from the mean\n\n## Standard Deviation and Z-Score\n2024-01-17T17:22:09.712Z\n- **Standard deviation**: A measure of the amount of variation or dispersion of a set of values. It illustrates how much individual data points differ from the mean.\n- **Z-Score**: A statistical measurement that describes a value's relationship to the mean of a group of values and quantifies how many standard deviations above or below the mean a data point is.\n- The discussion pertains to using Z-score values to determine the distance between data points and how it relates to clustering analysis.\n\n## Floating Point Numbers and Categorical Data\n2024-01-17T17:23:25.406Z\n- **Floating point numbers** are numbers that contain a decimal point. They can represent a wide range of values.\n- **Categorical data** refers to data that represents categories or groups, such as colors or types of fruit.\n\n## K-means Clustering\n2024-01-17T17:24:45.638Z\n- **K-means**: A popular unsupervised machine learning algorithm used for clustering data points into a pre-defined number of clusters.\n- Comparing data points to find clusters for poster creation, based on the nearest neighbors.\n- Exploring the process of creating clustered groups on the ground.\n\n## K Means Clustering\n2024-01-17T17:25:41.736Z\n- K Means Clustering: a type of unsupervised learning algorithm used to group similar data points into a fixed number (k) of clusters.\n- The algorithm iteratively assigns each data point to the nearest of k centroids and then recalculates the centroids based on the mean of the assigned data points.\n- Involves choosing the number of clusters, initializing centroids, assigning data points to the nearest centroid, and updating centroids.\n\n## Hierarchical Clustering\n2024-01-17T17:26:51.716Z\n- **Hierarchical clustering**: A method of cluster analysis which seeks to build a hierarchy of clusters. \n- Hierarchical clustering can be advantageous when working with small datasets, as it enables the formation of clusters from individual observations, but it may not be suitable for large datasets as it can become computationally intensive.\n- Outliers can greatly impact the output of hierarchical clustering. The presence of outliers can affect the distance measurements and ultimately influence the clustering process.\n\n## Hierarchical Clustering and Data Preprocessing\n2024-01-17T17:28:04.717Z\n- Hierarchical clustering: A method of cluster analysis that builds a hierarchy of clusters by either merging or splitting them successively.\n- Outliers removal: The process of eliminating data points that significantly differ from the rest of the data to improve analysis accuracy.\n- Handling large datasets: Hierarchical clustering may not be efficient for datasets with thousands or millions of data points.\n\n## Data Mining and Clustering Analysis\n- Data mining is the process of analyzing large data sets to identify patterns and relationships.\n- Cluster analysis is a technique used to group similar data points together based on certain characteristics.\n\n## Data Mining and Cluster Analysis\n2024-01-17T17:31:28.691Z\n- Data mining is the process of analyzing large sets of data to discover patterns or relationships.\n- Cluster analysis is a technique used to group similar items or data points together.\n\n## Converting Language to Numbers\n2024-01-17T17:40:33.694Z\n- **Unstructured Data**: Data that isn't organized in a pre-defined manner and doesn't have a specific data model. \n- Computer processing text requires converting it into structured numerical data for analysis.\n- The process involves identifying important information from the unstructured data to create a set of numbers for analysis.\n\n## Structuring Text Data for Analysis\n2024-01-17T17:44:11.693Z\n- We need to structure the text data to be something that can be plugged into SPSS, Rosen Collins, or equivalent tools.\n- We can create a term document matrix where important words are picked out and presented to the computer as a set of words.\n- A term document matrix allows us to pick out important words and provide this information to the computer, enabling analysis.\n\n## Structuring the Data for Analysis\n2024-01-17T17:44:11.693Z\n- Term Document Matrix: A way to represent the words in a document as rows and the documents themselves as columns. \n- SPSS and Rosen Collins: Refers to statistical analysis software used for processing survey data.\n- The speaker is discussing the process of creating a term document matrix to analyze survey responses.\n\n## Clustering of Textual Data\n2024-01-17T17:45:25.390Z \n- The process involves analyzing the frequency of words in a set of data to identify patterns and similarities.\n- Tokenization is the process of breaking text into individual words or tokens for analysis and is a crucial step in clustering textual data. \n\n## Tokenization and Text Preprocessing\n2024-01-17T17:46:50.004Z\n- **Tokenization**: Process of breaking text into individual words or tokens for analysis.\n- Remove non-word characters and punctuation marks from text data.\n- Convert all words to lowercase to ensure consistent analysis.\n- Identify and handle word endings to unify related terms.\n\n## Identifying and Processing Words\n- **Stacking**: The act of placing one item on top of another.\n- **Algorithm**: A step-by-step procedure for solving a problem or accomplishing a task, often used in computer programming and artificial intelligence.\n\n## Discussion on Natural Language Processing\n2024-01-17T17:48:05.422Z\n- Natural Language Processing (NLP) - a field of AI focused on enabling computers to understand, interpret, and manipulate human language.\n- Importance of standardizing text - converting all text to lowercase can help in simplifying the processing of data and reduce the variations due to upper/lower case.\n\n\n## Data Preprocessing and Text Analysis\n2024-01-17T17:49:07.485Z\n- Word Stemming - the process of reducing words to their root forms, such as converting \"stacking\" to \"stack\" and \"stacked\" to \"stack,\" to facilitate analysis.\n- Handling synonyms - discussing the transformation of similar words to a common representation, e.g., \"courteous\" and \"cordial\" both being transformed to \"polite,\" for more consistent analysis.\n- Removal of rare words - considering the insignificance of words occurring very infrequently, also referred to as outliers, to ensure meaningful and relevant data analysis.\n\n## Creation of Frequency Matrix\n2024-01-17T17:50:10.660Z\n- Creation of a frequency matrix - a tabular representation showing the occurrences of specific words (e.g., \"great,\" \"terrible\") in the dataset, aiding in data summarization and analysis.\n- Decisions based on word frequencies - using the frequency data to make decisions, such as understanding trends, progress, and changes over time based on the occurrences of certain terms.\n\n## Utilizing data for decision making\n2024-01-17T17:51:03.688Z\n- Decision-making based on data - leveraging the processed data to make informed decisions, possibly on a regular basis, to assess progress, trends, and changes in specific terms and overall data analysis.",
      "status": "inactive"
    },
    {
      "title": "Test 1.3",
      "note_id": "9dc4e6ec-6ef3-4c88-9a1a-5c9cc200b01d",
      "markdown": "#Test 1.3",
      "status": "active"
    }
  ]
}