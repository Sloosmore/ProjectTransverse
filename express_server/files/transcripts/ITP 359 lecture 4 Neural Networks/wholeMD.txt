# ITP 359 lecture 4 Neural Networks 

## Neural Networks and Approximators <2024-01-17T03:15:13.293Z>
- Neural networks as approximators - The neural network is used as an approximator by approximating a function with the help of learning from training examples.
- Basic idea of neural networks - Each node in the network uses a linear combination of feature vectors and associated weights.
- Vocabulary: 
  - **Approximator**: In machine learning and statistics, an approximator is a method or learning algorithm that seeks to approximate a target function through training on example inputs, and adjusting the model's parameters to minimize the difference between the predicted and actual outputs.

## Understanding the Perceptron Training Process
2024-01-17T03:20:32.067Z
- Perceptron training minimizes misclassifications
- Misclassifications are minimized using absolute value
- Weight updating is based on data points and their associated calculations
- Learning rate multiplied by the distance between the approximation and the actual value

## Training a model
2024-01-17T03:21:48.840Z
- Start with the weights being small, not too big to avoid issues with training.
- Perform steps for each data point, including calculating the linear combination, applying an activation function, and updating the weights.

## Perceptron Training Process 
2024-01-17T03:22:59.749Z
- Updating weights in each step for each data point
- Calculating the error and updating weights until a very small error is achieved

## Linearly Classifiable Data and Neural Networks
2024-01-17T03:25:44.088Z
- Linearly classifiable data: Data that can be separated into different classes using a straight line or hyperplane
- Neural networks: A series of algorithms that endeavors to recognize underlying relationships in a set of data through a process that mimics the way the human brain operates

## New AI Architecture
2024-01-17T03:27:15.177Z
- The new AI architecture on Netflix involves a dual network with multiple layers. 
- It utilizes activation functions such as sigmoid function, real or logistics function, and hyperbolic tan function to replace the step function.
- The architecture emphasizes the use of non-linearities in constructing multilayer neural networks.

## Neural Network Definition
2024-01-17T03:29:09.085Z
- The network is defined as starting at the end and working backward, with the output being an activation function of the output weights times the output of the final layer. 
- Each layer is defined by a number of "Lambda" nodes, with all nodes connected to each other, creating a fully connected feedforward network.

## Neural Network Structure
2024-01-17T03:32:14.821Z
- The output of a neural network depends non-linearly on the last layer, and this layer depends on the previous layer and so on, creating a recursive structure within the network.
- The network can be visualized as an onion, starting with the output and then working back to the input, with calculations being made as it progresses backward through the layers.

## Universal Approximation Theorem
2024-01-17T03:33:17.509Z
- The universal approximation theorem, first articulated in 1989, states that a feedforward neural network with a single hidden layer containing a finite number of neurons can approximate any continuous function on compact subsets.
- Compact subsets are subsets that include their boundary and do not have to satisfy specific constraints on their nature.

## Multi-class Classification and Softmax
2024-01-17T03:35:46.043Z
- The raw output of the network is passed through a sigmoid function, mapping the values to a range between 0 and 1, to obtain the probability for each class in a multi-class classification.
- The issue with using the sigmoid function alone is that the probabilities assigned to each class do not necessarily add up to one, hence creating ambiguity in classification.
- This method is referred to as multi-class classification, where multiple classes can be associated with the input.

## Sigmoid and Softmax Functions
2024-01-17T03:37:29.206Z
- Sigmoid function used in final layer for multi-class classification
    - Sigmoid function does not ensure probabilities add up to one
    - Allows for classification into multiple classes
- Softmax function exaggerates differences in probabilities and ensures sum of probabilities equals one
    - Enables exclusive belonging or mapping to a class

## Softmax and Multi-Class Classification
2024-01-17T03:38:30.789Z
- Softmax ensures that the output values add up to one, which is essential in binary classification to obtain a classification probability.
- In multi-class classification, you may want to compare probabilities between different outcomes, such as distinguishing between various objects in an image.
- Softmax exaggerates the distances between classes, making them not directly comparable, which might not be suitable if you want to rank the different classes based on their probabilities.

## Softmax and Cross Entropy Loss
2024-01-17T03:40:46.085Z
- Softmax: A function that takes as input a vector of K real numbers, and normalizes it into a probability distribution consisting of K probabilities.
- Softmax is used in neural networks to provide probabilities for the different classes in a classification problem.
- Cross Entropy Loss: A loss function used in neural networks when the output is a probability value. It measures the performance of a classification model whose output is a probability value between 0 and 1. 

## Cross Entropy Loss
2024-01-17T03:42:03.824Z
- Cross entropy loss is very similar to the negative log likelihood and is used in models.
- In the case of two classes, cross entropy loss becomes the same as the negative log likelihood equation.
- The derivation of cross entropy loss comes from taking the logarithm and maximizing the likelihood function.

