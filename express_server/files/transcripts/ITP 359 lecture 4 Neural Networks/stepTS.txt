2024-01-17T03:43:11.596Z> and again it comes from the from you know this this whole process of saying okay we're kind of like us to take the logarithm and then apply or maximize a lot the likelihood function and that's you know it means minimized negative block likelihood Etc that's how you do it arrive at the Crossing and we always use and we use as a final activation function the softmax here you know gives us out for probabilities that are all that add up to one we have a very clear winner in this and then you know minimize stuff<2024-01-17T03:44:19.344Z> here to find the parameters just a word the softmax is only applied in the final layer the other layers have any other activation parts and remember that and our code usually softmax is only for the final layer to map that to probabilities and then this is from my seat for my pieces by train the multim player Network there and it's actually very old the How to Train this thing is ready to send so we we discuss some of that already so you know you have a loss function for regression you know we use this mean square error right and classification we have crossed cross entropy and then<2024-01-17T03:45:24.519Z> is in a similar to or it's actually the same thing as the perceptron right just that depression has you know the derivatives result in an easier easier equation there for in our case we have to change the derivatives here in more General case of the loss function to update the weights derivative of the loss function are that is the big problem why these things take so long to train on your computer right calculating derivatives is is computationally expensive right and we have to take derivatives here towards all the way so there's a lot of derivatives to take and you have to do this for all database that's by the way why we use sarcastic right into something SGD that only samples and small number of data points<2024-01-17T03:47:29.458Z> and you basically stop when you reach the plateau right so when your derivative becomes 0 so you're basically at this point you now oscillate around this and song actually next time I'll show you some parameters to avoid that so there's a lot of problems with this you can you might not find the right anymore you might be stuck in a minimum that's going back to my stuff here you might be stuck in a small minute and not in the basement how do you know that big disadvantage of some afraid to send how you do that is there's somebody called boosting for example where you try to jump you just boost to some other place and then see what happens if you find a different way and sarcastic boosting it's called it there's another problem about oscillation so when this become that needs to become very small you oscillate round of the minimum there and that can be suppressed using momentum and I'll show you it's just a parameter that you add in the compiler so anyways it in principle this this works right so you try to find them minimum by adjusting and you know the weights using the negative of the negative slope here right going down towards the new moon but again it is very difficult or impossible to find the globalness you might be stuck in a smaller and many many smart people have put thought into how can you find the right or the best thing but in our case all the models were trained use their pain to send or sarcastic rate to send or Adam right you might have seen Adam is just just means that the learning rate is just<2024-01-17T03:48:29.350Z> so and then back propagation so I went to class everybody the derivatives in fact is derivatives here they are calculated using chain rule right so when you take a derivative off the of the Lost function towards a certain weight you you're basically you would you travel back to your network like we did before all the way to your input limit right that's called back propagation because the derivatives derivative of loss function in a certain layer depends on the layer before and it depends on labor before is that right it comes from the chain rule