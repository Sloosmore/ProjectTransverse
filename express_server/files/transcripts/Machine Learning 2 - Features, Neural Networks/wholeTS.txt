okay welcome back everyone this is the second lecture on machine learning so just before we get started a couple of announcements homework one foundations is due tomorrow at 11:00 p.m. note that it's 11:00 p.m. not 11:59 and please I would recommend everyone try to do a test mission early right it would be unfortunate if you wait until 10:59 and you realize that your computer you can log into the website if that happens please don't just email as much as you want before the deadline so there's no penalty to just submitting something and checking make sure you work yeah so just to remind you are responsible for any technical issues you encounter so please do the test mission early so you have a piece of mind and then you can go back to finishing your your homework okaywe do next Tuesday and finally there's a section this Thursday which will talk about back propagation and nearest neighbors and maybe overview of scikit-learn which might be useful for your project so please come to that okay so let's jump in I'm going to spend a few minutes review and if you unpack the learner we talked about how we want to frame it as an optimization problem which captures what we want to optimize what properties the predictor app should satisfy apart from the optimization algorithm which is how we accomplish our objective so the optimization problemsand in symbols this is the training loss which depends on a particular weight Vector is the average over all examples in a trading set of the loss of that particular example with respect to the way Vector W okay and we want to find the W that minimizes the training loss so we want to find the single W that make sure it's on average all the examples have low loss okay so looking at the loss functions now this is where it depends on what we're trying to do if we're doing regression then the permanent thing to look at is the residual which remember is the models prediction minus the true label so this is kind of how much we overshoot and the loss is going to be zero if the residual is zero and it increases either quadratically for the square loss or linearly for the absolute deviation depending on how much we want to penalize large deviations for classification or binary classification more specifically the permanent quantity look at is the margin which is the score times of the label Y which remember is plus one or minus one so the margin is a single number that captures how correct we are so large margin is good in that case we obtain either a 0 or near zero loss and margin lessons 0 means that we're making a mistake so the zero on Lost captures that we're making a mistake of the lost one but the hinge loss and religious question like I know