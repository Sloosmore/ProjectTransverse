technical content okay so let's get started with the actual technical content so remember from last time we gave it overview of the class we talked about different types of models that we were going to explore reflex model state-based models variable based models and logic models which will see throughout the course so in this lecture I'm going to start with the simplest of the models a reflex-based models and show how machine learning can be applied to these type of models and throughout the class we're going to talk about different types of models and how learning well help with those as well so there's going to be three parts we're going to talk about linear predictors which includes classification regression lossand then sarcastic gradient descent which is an algorithm that allows you to actually do the work so let's start with perhaps the most cliche example of you know machine learning so you have we want to do spam classification so the input is X an email message is a spam or not spam so we're going to denote and our goal is to produce a predictor F right so a predictor in general is going to be of a function that Maps some input X to some output Y in this case it's going to take an email message and map it to whether the email message is Spam or not okay so there's many types of prediction problems one where the output is one of two possibilities either yes or no and we're going to usually denote this as plus one or minus one sometimes you also see one and zero there's regression where you're trying to predict a numerical value for example let's say housing price there's a multi-classification where Y is not just two items but possibly 100 items maybe cat dog trucks is a permutationwhat pages and you want to rank them in some order to show to a user structure prediction is where why the output is an object that is much more complicated perhaps it's a whole sentence or even an image and it's something that you have to kind of construct you have to build the same from scratch it's not just a labeling and there's many more types of prediction problems but whenever someone says I'm going to do machine learning the first question so we're going to call an example X Y pair is something that specifies what the output should be when the input is X okay and a training data or a set of examples the training set is going to be simply a list or multi set of examples so you can think about this is a partial specification of behavior so rememberof what that system should do if I have some email messages of dollar signs then it might be spam and so remember this is not a full specification behavior these are 10 examples of what the function could do on those particular examples okay so once you have this data so we're going to use D train to denote the data set remember it's a set of input output pairs we're going to push this into a learning algorithm or a learner and what is the learning algorithm going to produce it's going to produce a predictor so predictors are F and the particular member is what it's actually itselfa map said to an output y okay so there's kind of two levels here and you can understand this in terms of the modeling inference learning Paradigm so mommy is about other question of what should the types of a predictors app you should consider are inference is about how do you compute why given X and learning is about how you take data and produce a predictor so that you can do inference any questions about this so far so this is pretty high level in abstract and generic right now and this is kind of on purpose because I want to highlight how General machine learning is before going into the specifics of linear predictors right so this is an abstract frameworkokay so let's dig in a little bit to this actual an actual problem so just a simplify the email problem let's consider a task of predicting whether a string is an email address or not so the input is an email or minus one if it's not that's where you want so the first step of doing linear prediction is known as feature extraction and the question you should ask yourself is what properties of the input acts might be relevant for predicting the output why right so I say it really highlight might be right at this point you're not trying to encodeyou know more of a back seat and you're saying well here's some hints that could help you okay so formally a feature extractor takes an input and outputs a set of feature name feature value pairs right so I'll go through example here so if I have gmail.com well you might consider the life of the string if it's greater than 10 maybe long strings are less likely to be emailed addresses in shorter ones and here are the feature name is length grade so that's just kind of the label of that feature and the value of that feature is one representing it's true so it would be zero if it's false there's another feature the fraction of alphanumerical charactersthere might be features that test for a particular your letters for example that is a container at sign or that as a you know future value of one because there isn't that sign ends with our communist one ends without zero because that's not true so and then you can have many many more features when we'll talk more about features on next time but the point is that you have a set of properties of okay so you have this feature Vector which has is a list of feature values and their Associated names or labels okay but later we'll see that the names don't matter to the learning algorithm so actually what you should also think about the feature Vector is simply a list of numbers and just kind of on the side make a note that oh this you know right so I've distilled the the email address abc@gmail.com into the list of numbers 0 or 1 0.85110 okay so that's a feature extraction it's kind of distilling complex objects into a list of numbers which we'll see is what the kind of the Bingo Franc of these machine learning algorithms has Okay so I'm going to write some Concepts on the board there's going to be a bunch of Concepts I'm going to introducefeature Vector is a kind of important notion and so noted fee of X on input so fee itself sometimes you think about it you call it the feature map which takes an input and returns a vector and this notation means that returns in general D dimensional Vector so list of D numbers and the components of this feature Vector we can write down feature all the way to Fiji of x okay this notation is your convenient because we're going to start shifting our Focus from thinking about the features as properties of input to Features as kind of mathematical objects so in particular fee of X is a point in high dimensional space so if you had two features that would be a point in two dimensional space but in general you might have a million features so that's a feature it's a point so it might be hard to think about that space But well we'll see how we can you know deal with that later in a bit