homework wine our deadline section tomorrow we're going to go through the back propagation example which I went through very briefly in the last lecture talk about dearest Neighbors which I did win one minute and also they're going to talk about cyclic learn which is this really useful tool for doing machine learning which might be useful for your final projects good to know so please come to section all right let's do a little bit of review of Where We Are so we've talked about we're talking about machine learning in particular supervised learning where we start with feature extraction where we take examples and convert them into a feature Vector which is more amenable for a learning algorithm we can take either linear predictors or neural networks which gives us scores and the score is either defined via simple or some sort of more fancy linear combination that gives us you know score functions which thing can be used for classification regression we also talked about lost functions as a way to assess the quality of a particular predictor so in linear classification we had the zero loss and the hinge loss as example of loss functions that we might care about the training loss is an average over the losses on individual examples and to optimize all of this we can use the stochastic gradient algorithm whichXY and a computer gradient on that particular example and then just updates know the weights based on that okay so hopefully this should be all no review okay so now I'm going to ask the following question no let's be a little bit philosophical here so what is the true objective you know of machine learning so how many of you think it is to minimize the error on the training center no one that's why we've been talking about right we've been talking about minimizing it on training sets Okay well maybe that's maybe that's not right then what about minimizing training with regularization because gregorization is probably a good idea tell me if you think that's that's the goal what about OK Googlewhat about minimizing air on unseen future examples okay so the majority of you think that's the right answer and what about learning about machines who doesn't want to learn about machines that's actually the true objective no so the correct answer is minimizing error on unseen future examples so I think all of you have the intuition that we are doing some machine learning we're learning on data but we really care about is how this predictor performs on in the future because we're going to deploy this in a system and it's going to be the future it's going to be unseen and then but then okay so then how do we do think about all these other things so that's going to be something we'll come back to laterwhen you're doing machine learning and then we're going to switch gears and talk about unsupervised learning where we don't have labels but we can still do something so we've been talking about training loss right now so the question is like is this training loss a good objective function well let's take this literally wanted to just minimize the training loss what would we do well here's an algorithm for you so you just store your training examples okay and then you're going to Define your predictor so if you see that particular example in your training set then you're just going to Output that way you saw in the train set and then otherwise right now